---
title: "Individual Survival Distributions - Case Study and Code Tutorial"
author: "Humza Haider"
date: "`r format(Sys.time(), '%B %e, %Y')`"
output:
  html_notebook: 
    toc: TRUE
    number_sections: true
    fig_caption: yes
    fig_align: "center"
---


This article will serve as a tutorial for the code which accompanies the paper "Effective Ways to Build and Evaluate Individual Survival Distributions" by Haider et al. The goal here is not only to show how one may make their own experiments but also address specific implementation details which would have been too cumbersome to include in the original paper. The outline of this tutorial is as follows: COME UP WITH SOMETHING.


#Introduction to the Example Data

The code here has been written for *survival* datasets, *i.e.* datasets are comprised of (1) a variable which indicates time of event, (2) an indicator variable for RIGHT censoring, and (3) features for each patient. Note that the code only allows for right censored patients, left censored or interval censored data will not work with the current implementation. 

As a case study we will use the Northern Alberta Cancer Dataset (NACD) which was also included in the original paper. However, to spice things up a bit we will use a new subset of the dataset, namely we will only consider patients with stage 4 stomach cancers. This data contains a total of 128 patients (100 uncensored) and 53 features. Below we give the summary statistics of the first five variables:

```{r}
NACD = read.csv("NACDS4SRAW.csv")
head(NACD)
summary(NACD[,1:5])
```


Here, we have "Time" as the variable indicating the time until the event occured and "Status" representing the indicator for death, that is, a 1 indicates the patient died and a 0 indicates the patient was right censored. To examine the other 48 features of the dataset in detail feel free to download the data which is freely avaliable (somewheres?). 

#Preproccesing the Data

##Validating and Cleaning  (validateAndClean.R)

**File Purpose**: This file is used to make sure the survival dataset is in the correct format for analysis. For example, the variable used for survival time must be named exactly **time**. Additionally, a minimal amount of data cleansing is completed in this file.

**File Path**: "ISDEvaluationPipeline/ValidateCleanCV/validateAndClean.R".

**Functions**:

1. *validateAndClean(survivalDataset, imputeZero=T)*
2. *validate(survivalDataset, imputeZero)*
3. *clean(survivalDataset)*

Note: You will notice this file structure throughout the codebase, there will often be a master function (here *validateAndClean()*) which runs the other functions included in the file, *e.g.* *validate()* and *clean()*. 

**Arguments**:

* *survivalDataset*: a (non-empty) dataframe representing the survival dataset -- *e.g.* NACD
* *imputeZero*: a boolean indicating if zero valued survival times should be imputed (see below for details).

**Packages Required**:

1. caret -- The *nearZeroVar()* function to identify variables with only 1 unique value and remove them in the *clean()* function.


###*validate(survivalDataset, imputeZero)*

The function *validate()* checks a number of things:

```{r, echo=FALSE }
validate = function(survivalDataset,imputeZero){
  if(is.null(survivalDataset) || nrow(survivalDataset) == 0)
    stop("You must enter a nonempty survival dataset.\n")
  if(!"time" %in% names(survivalDataset))
    stop("The variable 'time' in not included in the given dataset.\n")
  if(!"delta" %in% names(survivalDataset))
    stop("The variable 'delta' in not included in the given dataset.\n")
  survivalDataset$time = as.numeric(survivalDataset$time)
  survivalDataset$delta = as.numeric(survivalDataset$delta)
  #We want to change the ordering so that the time column is first and the delta column is second.
  timeIndex = which(names(survivalDataset) == "time")
  deltaIndex = which(names(survivalDataset) == "delta")
  survivalDataset = survivalDataset[,c(timeIndex,deltaIndex, (1:ncol(survivalDataset))[-c(timeIndex,deltaIndex)])]
  if(any(is.na(survivalDataset$time))){
    warning("'time' includes NA values. These rows have been removed.\n")
    NAtimeIndex = which(is.na(survivalDataset$time))
    survivalDataset = survivalDataset[-NAtimeIndex,]
  }
  if(any(is.infinite(survivalDataset$time))){
    warning("'time' includes Inf values. These rows have been removed.\n")
    InftimeIndex = which(is.infinite(survivalDataset$time))
    survivalDataset = survivalDataset[-InftimeIndex,]
  }
  if(any(survivalDataset$time <0)){
    warning("'time' includes negative values. These rows have been removed.\n")
    nonPosTimeIndex = which(survivalDataset$time <0)
    survivalDataset = survivalDataset[-nonPosTimeIndex,]
  }
  if(any(survivalDataset$time ==0) & imputeZero){
    imputeVal  =min(survivalDataset$time[survivalDataset$time > 0])/2
    warning(paste("'time' includes 0 valued times. These have been imputed to half the minimum positive time point:",
                  imputeVal,"\n"))
    zeroTimeIndex = which(survivalDataset$time ==0)
    survivalDataset[zeroTimeIndex,]$time = imputeVal
  }
  if(any(is.na(survivalDataset$delta))){
    warning("'delta' includes NA values. These rows have been removed.\n")
    NAdeltaIndex = which(is.na(survivalDataset$delta))
    survivalDataset = survivalDataset[-NAdeltaIndex,]
  }
  if(any(is.infinite(survivalDataset$delta))){
    warning("'delta' includes Inf values. These rows have been removed.\n")
    InfdeltaIndex = which(is.infinite(survivalDataset$delta))
    survivalDataset = survivalDataset[-InfdeltaIndex,]
  }
  if(length(unique(survivalDataset$delta)) >2)
    stop("'delta' contains more than two unique values. 'delta' should only contain 0 and 1.\n")
  if(!all(sort(unique(survivalDataset$delta)) == c(0,1)))
    stop("'delta' contains values other than 0 and 1. 0 should indicate RIGHT censoring and 1 should indicate the event occuring.\n")
  return(survivalDataset)
}
```

1. A survival dataset was nonempty.
2. The survival time variable MUST be named **time**.
3. Similarly, the indicator of death variable must be named **delta**. The naming choice of this variable is the common notation in survival analysis.
4. If **time** includes NA values these rows will be removed.
5. If **time** includes Inf values these rows will be removed.
6. If **time** includes negative values these rows will be removed.
7. If **time** includes zero values there are two options depending on the value of the argument imputeZero. If imputeZero = TRUE, then zero valued times will be imputed to half the minimum non zero valued time. For example in the NACD dataset we are using here the minimum time is 0.2333. If there was another row where the event time was 0, this would be imputed to $\frac{0.2333}{2} = 0.1166$. We include this option as some survival models (*e.g.* Accelerated Failure Time with the Weibull distribution) will fail if given a zero valued time. If imputeZero = FALSE then nothing will happen to these zero valued survival times. Currently there is no option to remove zero valued times other than removing them from the dataset prior to running the code avalibale here.
8. If **delta** includes NA values these rows will be removed.
9. If **delta** includes Inf values these rows will be removed.
10. **delta** must only have two unique values.
11. The unique values of **delta** must be 0 and 1 -- 0 for censored data and 1 for uncensored data.

If we try to use *validate* on our NACD dataset we will get an error since currently our survival time variable is named "Time" and our death indicator is named "Status". Note that below we include imputeZero = TRUE, but since there are no zero-valued times the same result would occur if we included imputeZero=FALSE.

```{r, error= TRUE}
validatedNACD = validate(NACD, imputeZero = TRUE)
names(NACD)[1] = "time"
validatedNACD = validate(NACD, imputeZero = TRUE)
names(NACD)[2] = "delta"
validatedNACD = validate(NACD, imputeZero = TRUE)
```

###*clean(survivalDataset)*

```{r, echo = FALSE,message=FALSE}
library(caret)
clean = function(survivalDataset){
  #Remove columns with no variance.
  allSame = nearZeroVar(survivalDataset,freqCut = 100/0)
  if(length(allSame)>0){
    namesToRemoveNoVar = names(survivalDataset)[allSame]
    survivalDataset = survivalDataset[,-allSame]
    warning(paste("The variable(s)",paste(namesToRemoveNoVar, collapse = ", "),"contained only 1 unique value. They have been removed.\n",
                  sep = " "))
  }
  #Remove columns with over 25% of data missing.
  naProp = apply(survivalDataset, 2, function(x) sum(is.na(x))/nrow(survivalDataset))
  toRemoveNA = which(naProp > 0.25)
  if(length(toRemoveNA)>0){
    namesToRemoveNA = names(survivalDataset)[toRemoveNA]
    survivalDataset = survivalDataset[,-toRemoveNA]
    warning(paste("The variable(s)",paste(namesToRemoveNA, collapse = ", "),"contained over 25% NA values. They have been removed.\n",
                  sep = " "))
  }
  indicatorVariables = sapply(survivalDataset[,-c(1,2)], function(x) all(unique(x)[!is.na(unique(x))] %in% c(0,1)) || all(unique(x)[!is.na(unique(x))] %in% c(-1,0)))
  if(any(indicatorVariables)){
  survivalDataset[,-c(1,2)][,indicatorVariables] = lapply(survivalDataset[,-c(1,2)][,indicatorVariables], factor)
  warning(paste(toString(names(survivalDataset[-c(1,2)][indicatorVariables])), "had only unique values c(0,1) or c(-1,0). These were converted to factors.",sep = " "))
  }
  return(survivalDataset)
}
```


Here, *clean()* does a very minimal amount of cleaning on the survival dataset. Specifically, the goal is to remove variables which will be not be meaningful to analysis. There are two cases where variables are removed:

1. A variable contains all unique values. For example, in our NACD dataset there is a **STAGE_4** variable which is an indicator (1 or 0) whether or not the cancer is in stage 4. Since we only have stage 4 stomach cancer patients this variable will be be all 1s and will be removed from the dataset.
2. Variables such that 25% of rows (patients) have a missing value will be removed. However, the coding of missing data must be such that is.na() would identify the data as missing. For example is.na("") returns TRUE even though an empty string likely indicates a missing value.
3. Often if a factor (nominal) variable only contains two values (say Male and Female) individuals will not change this to explicitly be a factor and leave the variable in terms of 1s and 0s. However, if these variables also contain missing values this will create weird behaviors when doing imputations, e.g. a missing value would be imputed as the average gender *e.g* 0.7 which is meaningless. For this reason we MAKE THE ASSUMPTION that all variables which only contain non-NA unique values of c(0,1) or c(0,-1) are actually factor variables and this change is made to the data.

Whenever a variable is removed or turned into a factor, *clean()* will return a warning indicating the name of the variable(s) which were removed/factored. For example, on our validated NACD dataset we can now call *clean()*:


```{r}
cleanedNACD = clean(validatedNACD)
```
As noted above a number of variables were removed due to having only 1 unique value. 


###*validateAndClean(survivalDataset, imputeZero=T)*

```{r, echo = FALSE,message=FALSE}
validateAndClean = function(survivalDataset, imputeZero=T){
  validatedData = validate(survivalDataset, imputeZero)
  cleanData = clean(validatedData)
  return(cleanData)
}
```


This function simply runs both *validate()* and *clean()* on a survivalDataset. Note that here we default imputeZero to be TRUE. Below we verify that *validateAndClean(NACD)* is equivalent to cleanedNACD.

```{r, warning=FALSE}
all(validateAndClean(NACD) == cleanedNACD)
```


##Creating Cross-Validation Folds and Normalizing Features (createFoldsAndNormalize.R)

**File Purpose**: This file is used to create folds from the survival dataset for cross validation and normalize features. As part of normalization, mean/mode imputation is employed to handle missing data.

**File Path**: "ISDEvaluationPipeline/ValidateCleanCV/createFoldsAndNormalize.R".

**Functions**:

1. *createFoldsAndNormalize(survivalDataset, numberOfFolds)*
2. *createFoldsOfData(survivalDataset, numberOfFolds)*
3. *meanImputation(listOfDatasets)*
4. *normalizeVariables(listOfImputedDatasets)*


**Arguments**:

* *survivalDataset*: the survivalDataset AFTER going through validation and cleaning -- *e.g.* cleanedNACD
* *numberOfFolds*: the desired number of cross-validation folds, must be greater than 1.
* *listOfDatasets*: a list containing a list of Training datasets and a list of Testing datasets - *i.e.* the output of *createFoldsOfData()*
* *listOfImputedDatasets*: same as *listOfDatasets* but missing values have been imputed (the output of *meanImputation()*)

**Packages Required**:

1. caret -- The *dummyVars()* function is used to create a one hot encoding of factor (nominal) variables.
2. dataPreparation -- The *build_scales()* and *fastScale()* function are used to normalize variables.

###*createFoldsOfData(survivalDataset, numberOfFolds)*{#folds}
```{r, echo=FALSE}
createFoldsOfData = function(survivalDataset, numberOfFolds){
  #Create folds with the equal amounts of censoring and quasi equal ranges. Here we order by censor status and time to event
  #and then match these off to folds one by one. Doing it this way makes cross validation deterministic which is not ideal
  #but lets us have equally matched folds.
  time = survivalDataset$time
  delta = survivalDataset$delta
  Order= order(delta,time)
  foldIndex = lapply(1:numberOfFolds, function(x) Order[seq(x,nrow(survivalDataset), by = numberOfFolds)])
  listOfTestingSets = lapply(foldIndex, function(indexs) survivalDataset[indexs,])
  listOfTrainingSets = lapply(foldIndex, function(indexs) survivalDataset[-indexs,])
  listOfDatasets = list(Training = listOfTrainingSets, Testing = listOfTestingSets)
  return(list(foldIndex,listOfDatasets))
}
```
This code base requires that data be in cross-validation format with a minimum of 2 folds. The cross-validation used here is specific to survival analysis in that cross-validation is deterministic -- rows are first stratefied by their censoring status (**delta**) and then sorted by **time** and sequentially placed into folds (see Figure 1). The reasoning behind this procedure is two-fold (pun-intended):

1. The number of censored patients should be (roughly) equal across all folds.
2. The range of event times should also be (roughly) equal across all folds.

By creating folds in this way models will be tested on data which will be mostly representitive of what they saw in the training data.

![Figure 1: Depiction of 3-fold cross-validation scheme used for survival data. The dataset is first stratefied by delta and then sorted by time. Rows are then sequentially ordered into the three folds.](./SurvivalCV.png)

In addition to creating cross validation folds, *createFoldsOfData()* also returns the indexs of the rows that each test set belongs. See the below example for details. Here we put our cleaned NACD dataset (cleanedNACD) into 5 folds:

```{r}
folds = createFoldsOfData(cleanedNACD, 5)
#folds comprises a list of (2) items. (1) is the indexs of the test set. For example, examine the first test set fold:
testSetIndexs = folds[[1]]
#Test fold 1:
testSetIndexs[[1]]
```
Here the first test fold is comprised of the 123rd, 101st, 62nd, ..., and the 77th rows of the original data. We will later use these indexs for plotting in Section ??????.

The second item returned by *createFoldsOfData()* is a list of (1) the training sets, and (2) the testing sets. Below we simply give the dimension of each:
```{r}
trainingAndTestingSets = folds[[2]]
#First Training Set:
dim(trainingAndTestingSets$Training[[1]])
#First Testing Set:
dim(trainingAndTestingSets$Testing[[1]])
```

###*meanImputation(listOfDatasets)*

```{r, include = FALSE}
meanImputation = function(listOfDatasets){
  #Here we take the means (numeric variables) and modes (factor variables) of the training and data and impute the training AND the 
  #testing data with the same mean/mode of the respective training data.
  for(i in 1:length(listOfDatasets$Training)){
    train = listOfDatasets$Training[[i]]
    test = listOfDatasets$Testing[[i]]
    #Note that we are also imputing time and delta, but validation removed all NA instances of time and delta so we are really imputing 
    #nothing.
    
    #For numeric variables:
    trainNumeric = train[,sapply(train,is.numeric), drop = FALSE]
    #drop = FALSE makes it stay as a dataframe instead of turning to a vector in the event there is only 1 variable with the condition.
    testNumeric = test[,sapply(test,is.numeric), drop = FALSE]
    varMeans = apply(trainNumeric,2,function(x) mean(x, na.rm = T))
    trainNumericImputed = apply(trainNumeric, 2, function(x) ifelse(is.na(x),mean(x, na.rm = T),x))
    #The function is more complex for test because we have to specify the means and sd of the TRAINING set as we go through 
    #the columns of the test set.
    testNumericImputed = as.data.frame(sapply(1:length(varMeans), function(x) ifelse(is.na(testNumeric[,x]),varMeans[x],testNumeric[,x])))
    #Names are getting lost in sapply
    names(testNumericImputed) = names(testNumeric)
    
    
    #For factor variables:
    trainFactor = train[,sapply(train,is.factor),drop=FALSE]
    testFactor = test[,sapply(test,is.factor),drop=FALSE]    
    if(ncol(trainFactor) > 0){
      #We need to introduce a mode function to find the mode of the factor variables.
      Mode <- function(x) {
        #Modified from https://stackoverflow.com/questions/2547402/is-there-a-built-in-function-for-finding-the-mode
        x = x[!is.na(x)]
        ux <- unique(x)
        ux[which.max(tabulate(match(x, ux)))]
      }
      varModes = apply(trainFactor,2,function(x) Mode(x))
      trainFactorListed = lapply(trainFactor, as.character)
      trainFactorImputed = as.data.frame(lapply(trainFactorListed, function(x) ifelse(is.na(x), Mode(x), x)))
      #We run into problems if the training dataset didn't have a factor level in it which was included in the 
      #original dataset. Here we are just adding back the original factor levels.
      for(j in 1:ncol(trainFactorImputed)){
        missedLevels = levels(trainFactor[,j])[which(!levels(trainFactor[,j]) %in% levels(trainFactorImputed[,j]))]
        levels(trainFactorImputed[,j]) = c(levels(trainFactorImputed[,j]), missedLevels)
      }
      testFactorImputed = as.data.frame(sapply(1:length(varModes),
                                               function(x) factor(ifelse(is.na(testFactor[,x]),
                                                                         varModes[x],
                                                                         paste(testFactor[,x])),
                                                                  levels = levels(trainFactorImputed[,x]))))
      names(testFactorImputed) = names(testFactor)
      #Combine numeric and factor variables and save over the previous datasets.
      listOfDatasets$Training[[i]] = cbind.data.frame(trainNumericImputed, trainFactorImputed)
      listOfDatasets$Testing[[i]] = cbind.data.frame(testNumericImputed, testFactorImputed)
    }
   else{
     listOfDatasets$Training[[i]] = as.data.frame(trainNumericImputed)
     listOfDatasets$Testing[[i]] = as.data.frame(testNumericImputed)
   }
  }
  return(listOfDatasets)
}
```

Following the splitting of folds we perform imputation for missing data. Since the primary focus of this work was not in any way geared towards imputation methods we do a very simple mean/mode imputation for missing values WITHIN FOLD. The means for all numerical variables for each training fold are computed and the corresponding test fold is imputed with the mean value from the training fold. Similarly, factor variables are imputed with the mode factor level. Note that by either imputing missing values for the entire dataset or imputing training and test folds seperatly would lead data leakage. Note that **time** and **delta** are not subject to imputation -- if either of these variables contained missing values the entire row was removed in *validate()*.

Below we would impute missing data, however, the NACD dataset we are using has no missing data so the following step is not necasarry.
```{r, echo=TRUE,include=TRUE}
imputedData = meanImputation(trainingAndTestingSets)
```

###*normalizeVariables(listOfImputedDatasets)*
```{r, include=FALSE}
library(dataPreparation)
normalizeVariables = function(listOfImputedDatasets){
  listOfNormalizedDatasets = list()
  for(i in 1:length(listOfImputedDatasets$Training)){
    train = listOfImputedDatasets$Training[[i]]
    test = listOfImputedDatasets$Testing[[i]]
    #We need to remove time an delta so they don't get normalized. Here we set them apart and make an indicator for their location.
    timeDeltaInd = which(names(train) %in% c("time","delta"))
    timeDeltaTrain = train[,timeDeltaInd]
    timeDeltaTest = test[,timeDeltaInd]
    
    #fullRank =T drops one of the extra columns for the one-hot encoding.
    oneHotEncoder = dummyVars("~.",data = train[-timeDeltaInd], fullRank = T)
    trainEncoded = predict(oneHotEncoder, train[-timeDeltaInd])
    testEncoded = predict(oneHotEncoder, test[-timeDeltaInd])
    
    scales = build_scales(trainEncoded, verbose = F)
    trainCentered = cbind.data.frame(timeDeltaTrain,fastScale(trainEncoded, scales=scales, verbose=F))
    testCentered = cbind.data.frame(timeDeltaTest,fastScale(testEncoded, scales=scales,verbose=F))
    
    #If a variable had zero variance in the test set we will end up dividing by 0 and getting NaN values.
    #Here we simply make everything 0, the mean value, if this occurs.
    trainCentered = apply(trainCentered,2, function(x){
      if(all(is.nan(x))) 
        return(rep(0,length(x)))
      return(x)
    })
    testCentered = apply(testCentered,2, function(x){
      if(all(is.nan(x))) 
        return(rep(0,length(x)))
      return(x)
    })

    listOfNormalizedDatasets$Training[[i]] = as.data.frame(trainCentered)
    listOfNormalizedDatasets$Testing[[i]] = as.data.frame(testCentered)
    #We need to make sure that there is no comma in any of the file names or this can wreck functions using csvs.
    #We will search for these and remove them. Ideally we could do this once for the entire dataset but this would require
    #checking all variables and then all the levels of all the factor variables. Since this is linear in the number of 
    #features this check shouldn't be computationally difficult so we will do the "lazy" way and check for commas here 
    #and remove them for every fold.
    names(listOfNormalizedDatasets$Training[[i]]) = make.names(names(listOfNormalizedDatasets$Training[[i]]),unique=T)
    names(listOfNormalizedDatasets$Testing[[i]]) = make.names(names(listOfNormalizedDatasets$Testing[[i]]),unique=T)
    }

  return(listOfNormalizedDatasets)
}
```


Following variable imputation, all the variables are normalized (except **time** and **delta**) by taking the scales (mean and standard deviation) of the training set variables and normalizing both the training set and test set with those scales. Here normalizing follows the standard convention of subtracting the mean and dividing by the standard deviation. This function also does some cleaning of variable names -- if a variable name has a comma in it functions later of will break so all commas and other special characters are removed.

Below we normalize the data and show the difference between the data before and after it was normalized.
```{r}
head(imputedData$Training[[1]])
normalizedData = normalizeVariables(imputedData)
head(normalizedData$Training[[1]])
```

###*createFoldsAndNormalize(survivalDataset, numberOfFolds)*

```{r, include=FALSE}
createFoldsAndNormalize = function(survivalDataset, numberOfFolds){
  folds = createFoldsOfData(survivalDataset, numberOfFolds)
  originalIndexing = folds[[1]]
  listOfDatasets = folds[[2]]
  listOfImputedDatasets = meanImputation(listOfDatasets)
  listOfNormalizedDatasets = normalizeVariables(listOfImputedDatasets)
  return(list(originalIndexing, listOfNormalizedDatasets))
}
```

Similar to *validateAndClean()* above, *createFoldsAndNormalize()* serves as a master function which will run *createFoldsOfData()*, *meanImputation()*, and *normalizeVariables()* and will return a list of (2) items, (1) the indexs of the test set as given in Section 2.2.1, and (2) a list containing: (1) a list of normalized training folds and (2) a list of normalized testing folds. 

Here we show the equivalence between *testSetIndexs* from above and the first item of *createFoldsAndNormalize()*  and  equivalence of *normalizedData* with the second item of *createFoldsAndNormalize()*:

```{r}
output = createFoldsAndNormalize(cleanedNACD,5)
all.equal(testSetIndexs, output[[1]])
all.equal(normalizedData, output[[2]])
```

#Creating Individual Survival Distribution  Models

The intention of this work is to examine different survival analysis models which can produce individualized survival distirbutions for different patients. In total we explored five models which produced individual survival distributions and one model which gave a population based survival curve (Kaplan-Meier). Below we give a minimal description for each model and describe how to implement each in the current codebase.


##Kaplan-Meier (KaplanMeier.R)

The Kaplan-Meier (KM) estimator was devised as a nonparametric approach to estimating a survival curve even in the presence of censored data (see the original paper [here](https://web.stanford.edu/~lutian/coursepdf/KMpaper.pdf)). To give the KM estimator we first make the following definitions:

* \(\tau_j\) -- the \(j^\textrm{th}\) distinct death time,
* \(d_j\) -- the number of deaths at time \(\tau_j\),
* \(r_j\) -- the number of patients who were at risk at time \(\tau_j\). Note this also includes those who died at \(\tau_j\).

With these definitions, we say the probability of survival at time \(t\) is given by the KM estimator as
\[\hat{S}_{KM}(t) = \prod_{j: \tau_j < t} \left(1 - \frac{d_j}{r_j}\right)\].
Note that here KM does not take into account any features and thus is considered to a nonparametric model. To give an example of the Kaplan-Meier approach consider the following data:

```{r, echo = FALSE}
library(knitr)
library(kableExtra)
time = c(1,3,5,7,9)
delta = c(1,0,1,1,1)
dat = cbind.data.frame(time,delta)
row.names(dat) = c("Patient 1","Patient 2", "Patient 3","Patient 4","Patient 5")
kable(t(dat), caption = "Example data for the Kaplan-Meier model. Note the data is transposed from the normal format.", format = "html", align = c('c','c','c')) %>%
    kable_styling(bootstrap_options = "striped", full_width = F )
```

Given the data above, we have \(\hat{S}_{KM}(1) = \left(1 - \frac{1}{5}\right) = 0.8\). Note that no change occurs at \(t= 3\) since this patient was censored and \(\tau_j\) only refers to unique *death* times. Below we give the full KM curve for this example.

<center>
```{r, echo = FALSE,message=FALSE, fig.height=3}
library(survival)
library(ggplot2)
library(survminer)
fit = survfit(Surv(time,delta)~1, data = dat)
p = ggsurvplot(fit, data = dat, conf.int = F, legend = "none", palette = "black", censor.size = 10)
p$plot 
```
<figcaption>Figure 2: Kaplan-Meier plot using example data. Censored patients indicated by vertical dash on survival curve.</figcaption>
</center>

Since the KM model does not account for individual features, the survival distribution generated is supposed to represent the population's overall survival curve, *i.e.* the average survival curve. So given \(K\)-fold cross-validation, there will only be \(K\) different survival curves as opposed to the \(N\) different survival curves the other models generate, given \(N\) unique patients. Next we give the details for running KM on a dataset after it has preproccessed by *validateAndClean()* and *createFoldsAndNormalize()*.

###Kaplan-Meier Code (KaplanMeier.R)

```{r, echo =FALSE}
library(prodlim)

KM = function(training, testing){
  kmMod = prodlim(Surv(time,delta)~1, data = training)
  trainingTimes = c(sort(unique(training$time)))
  if(0 %in% trainingTimes){
    times = trainingTimes
  } else{
    times = c(0,trainingTimes)
  }
  probabilities = predict(kmMod,times)
  curvesToReturn = cbind.data.frame(time = times,matrix(rep(probabilities,nrow(testing)),ncol = nrow(testing)))
  timesAndCensTest = cbind.data.frame(time = testing$time, delta = testing$delta)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))   
}
```

**File Purpose**: Use the Kaplan-Meier model that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/KaplanMeier.R".

**Functions**:

1. *KM(training, testing)*

**Arguments**:

* *training*: The training set.
* *testing*: The testing set.

**Packages Required**:

1. survival -- We require the *Surv()* function to change the data into a survival format.
2. prodlim -- prodlim contains a Kaplan-Meier implementation that includes a *predict()* function.

**File Notes**:

Given the list of datasets generated by *createFoldsAndNormalize()*, *KM()* will take a single training and testing set and build the Kaplan-Meier curve for the training set using the *prodlim()* function from the *prodlim* package. The resulting Kaplan-Meier curve is the "predicted" survival distribution for all the testing patients. In addition to the survival curves, *KM()* will also return **time** and **delta** of the testing and training sets. The reason for these additional returned items is for evaluations given in Section ???. 

**Example Usagw**

Here, we give an example of using KM on a single training/testing fold -- extending the results to all folds can be accomplished by using a for loop. 


```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
#Recall the first argument of folds is the indexs of the test set on validatedAndCleaned and the second argument is the list of training and testing sets.
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
kmMod = KM(training1stFold, testing1stFold)
curves = kmMod[[1]]
testDat = kmMod[[2]]
trainDat = kmMod[[3]]
curves
```

Above you will see that the survival curves for all patients are the exact same -- the desired effect for the Kaplan-Meier model. Note that **time** will correspond to all the event times (both death and censored event times) of the *training* data. Additionally we include a **time** = 0 point where the survival probability will be 1 -- this is consistent across all models. 

For a more picturesque approach, we give the survival curve below:

<center>
```{r, echo = FALSE}
library(ggplot2)
library(reshape2)
plotSurvivalCurves = function(survivalCurves, indexToPlot = 1, color = c(), xlim = c()){
  colorOK = T
  if(length(color) == 0)
    colorOK = F
  else if(length(color) != length(indexToPlot)){
    warning("If you would like to select custom colors please make sure the number of colors
            matches the number of curves.")
    colorOK =F 
  }
  time = survivalCurves$time
  curves = survivalCurves[,indexToPlot +1,drop=F]
  plotTimes = seq(min(time),max(time), length.out = length(time)*20)
  plotProbs = as.data.frame(sapply(curves,
                                   function(curve){
                                     curve = ifelse(curve < 1e-20,0,curve)
                                     survivialSpline = splinefun(time, curve, method = "hyman")
                                     return(pmax(survivialSpline(plotTimes),0))
                                   }
  ))
  data = cbind.data.frame(plotTimes,plotProbs)
  longFormData = melt(data,measure.vars = names(data)[-1], variable.name = "Patient")
  plot = ggplot(data = longFormData, aes(x = plotTimes,y = value, colour = Patient))+
    geom_line(size = 1.5)
  if(colorOK)
    plot = plot + scale_color_manual(values = color) 
  if(length(xlim)==2){
    plot = plot+ xlim(c(xlim[1],xlim[2]))
  }
  plot = plot +scale_y_continuous( limits= c(0,1),breaks = c(0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1))+
    theme_bw() +
    theme(text = element_text(size=18, face=  "bold"),
          axis.title = element_text(size = 20),
          axis.title.x = element_text(margin = margin(t = 15)),
          axis.title.y = element_text(margin = margin(r = 15))) + 
    labs(y = "Survival Probaility",x = "Time" )

  return(plot)
}
plotSurvivalCurves(curves, 1:10)
```
</center>

One should notice that the survival curve does not drop to zero and instead ends at \(\approx 0.05\) -- this is because the last patient in the training data was censored. Additionally, the above plot does not follow the classical Kaplan-Meier step function and is smoother using a spline fit -- the details can be found in Section ??? which describes the plotting function.

So far we have shown the output of *curves* but ignored *trainDat* and *testDat* which are just the training and testing values for **time** and **delta**. Across all the following models, *trainDat* and *testDat* will be the same thing, but for clarity we will include them below and not in following models.

<center>**trainDat**</center>
```{r,echo=FALSE}
kable(trainDat, format = "html", align = c('c','c','c')) %>%
    kable_styling(bootstrap_options = "striped", position = "center") %>% 
      scroll_box(width = "65%", height = "200px",extra_css = "margin: auto;") 
```

<br><br>

<center>**testDat**</center>

```{r, echo=FALSE}
kable(testDat, format = "html", align = c('c','c','c')) %>%
    kable_styling(bootstrap_options = "striped", position = "center") %>% 
      scroll_box(width = "65%", height = "200px",extra_css = "margin: auto;") 
```

##Accelerated Failure Time 

The Accelerated Failure Time (AFT) has been a commonly used survival analysis technique for finding significant features in a survival seting. The model is motivated by assuming survival time, \(T\) follows some distribution with density function \(f(t)\) and survival function \(S(t)\). Common AFT distributions are log-logistic, Weibull, log-normal, exponential and more. While we make assumptions on the distribution of \(T\), inference is instead made on \(\log(T)\), that is, for a patient, \(\mathbf{x_i}\) where \(i = 1,2,\ldots N\), AFT models follow the equation:
\[
log(T_i) = \boldsymbol{w}^T\boldsymbol{x_i} + \sigma\epsilon_i,
\]
where \(\boldsymbol{w}\) are feature weights, \(\sigma\) is called a scale parameter and \(\epsilon_i\) is an independent, identically distributed noise parameter dependent on the assumed distribution of \(T\). This is essentially a normal linear regression but on the log scale of \(T\), *i.e.* features are assumed to have a linear relationship with the log of survival time. Here, \(\sigma\) is assumed to be constant across all patients and can be learned by maximum likelihood along with the weights \(\boldsymbol{w}\). 

###Accelerated Failure Time Code (AcceleratedFailureTime.R)
```{r, include=FALSE}
library(survival)

AFT = function(training, testing, AFTDistribution){
  tryCatch({
    AFTMod = survreg(Surv(time,delta)~., data = training, dist = AFTDistribution)
    
    trainingTimes = sort(unique(training$time))
    if(0 %in% trainingTimes){
      timesToPredict = trainingTimes
    } else {
      timesToPredict = c(0,trainingTimes)
    }
    survivalCurves = survfunc(AFTMod, newdata = testing, t = timesToPredict)
  },
  error = function(e) {
    message(e)
    warning("AFT failed to converge.")
  })
  if(!exists("AFTMod") | !exists("survivalCurves")){
    return(NA)
  }

  probabilities = survivalCurves$sur
  #Since survfunc returns survival probabilities with the first time point for every individual (ordered by how the testing individuals)
  #were passed in, we can simply fill a matrix by row to have each individual curve be a column. This can be verified by checking
  #the survival probabilties (sur) for any ID_SurvivalCurves against any column, e.g. the survival probabilities for ID_SurvivalCurves == 2,
  #correspond to the probabilites found in the second column of the matrix below.
  probabilityMatrix = matrix(probabilities, ncol = nrow(testing),byrow = T)
  curvesToReturn = cbind.data.frame(time = timesToPredict, probabilityMatrix)
  timesAndCensTest = cbind.data.frame(time = testing$time, delta = testing$delta)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))   
}

#The following was taken and altered from http://rstudio-pubs-static.s3.amazonaws.com/161203_6ee743eb28df4cd68089a519aa705123.html.
#This code is used to pass in a time point and get the predicted probability. The predict function for survreg objects only return 
#times from probabilities so we need to reverse enginner this using the code below.
survfunc = function (object, t, newdata, name = "t") {
  #Altered from origina: I am going to add an ID to every row so we can retrieve the individuals easily from the output.
  #I gave a weird ID variable name so if the original data came in with a variable ("ID") it won't break our system.
  newdata$ID_SurvivalCurves = 1:nrow(newdata)
  newdata <- do.call(rbind, rep(list(newdata), length(t)))
  t <- rep(t, each = nrow(newdata)/length(t))
  if (class(object) != "survreg") 
    stop("not a survreg object")
  lp <- predict(object, newdata = newdata, type = "lp")
  if (object$dist %in% c("weibull", "exponential")) {
    newdata$pdf <- dweibull(t, 1/object$scale, exp(lp))
    newdata$cdf <- ifelse(t == 0,0,
                          ifelse(is.nan(pweibull(t, 1/object$scale, exp(lp))),1,pweibull(t, 1/object$scale, exp(lp))))
    newdata$haz <- exp(dweibull(t, 1/object$scale, exp(lp), 
                                log = TRUE) - pweibull(t, 1/object$scale, exp(lp), 
                                                       lower.tail = FALSE, log.p = TRUE))
  }
  else if (object$dist == "lognormal") {
    newdata$pdf <- dlnorm(t, lp, object$scale)
    newdata$cdf <- plnorm(t, lp, object$scale)
    newdata$haz <- exp(dlnorm(t, lp, object$scale, log = TRUE) - 
                         plnorm(t, lp, object$scale, lower.tail = FALSE, log.p = TRUE))
  }
  else if (object$dist == "gaussian") {
    newdata$pdf <- dnorm(t, lp, object$scale)
    newdata$cdf <- pnorm(t, lp, object$scale)
    newdata$haz <- exp(dnorm(t, lp, object$scale, log = TRUE) - 
                         pnorm(t, lp, object$scale, lower.tail = FALSE, log.p = TRUE))
  }
  else if (object$dist == "loglogistic") {
    newdata$pdf <- dlogis(log(t), lp, object$scale)/t
    newdata$cdf <- plogis(log(t), lp, object$scale)
    newdata$haz <- exp(dlogis(log(t), lp, object$scale, log = TRUE) - 
                         log(t) - plogis(log(t), lp, object$scale, lower.tail = FALSE, 
                                         log.p = TRUE))
  }
  else if (object$dist == "logistic") {
    newdata$pdf <- dlogis(t, lp, object$scale)
    newdata$cdf <- plogis(t, lp, object$scale)
    newdata$haz <- exp(dlogis(t, lp, object$scale, log = TRUE) - 
                         dlogis(t, lp, object$scale, lower.tail = FALSE, log.p = TRUE))
  }
  else {
    stop("unknown distribution")
  }
  newdata$sur <- 1 - newdata$cdf
  newdata[name] <- t
  return(newdata)
}
```
**File Purpose**: Apply the Accelerated Failure Time model (for a number of distributions) to data that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/AcceleratedFailureTime.R".

**Functions**:

1. *AFT(training, testing, AFTDistribution)*
2. *survfunc(object,t,newdata, name)* - Used to extract survival cures from an AFT model. Altered from [a function previously written by Timothy Johnson.](http://rstudio-pubs-static.s3.amazonaws.com/161203_6ee743eb28df4cd68089a519aa705123.html)


**Arguments**:

* *training*: The training set.
* *testing*: The testing set.
* *AFTDistribution*: The distribution for the AFT model. Possible distributions: *exponential, weibull, lognormal, gaussian, loglogistic, logistic*.
* *object*: A *survreg* object from the *survreg()* function.
* *t*: The times for which to evaluate the AFT model.
* *newdata*: The testing data to predict with using the AFT model.
* *name*: The name of the variable containing the predicted times (*t*).

**Packages Required**:

1. survival -- We use the *survreg()* function to perform analysis for AT models.

**File Notes**:

The primary function for this file is *AFT()*; *survfun()* is a helper function AFT uses to build survival curves for test data after the model has been learned. 

Given the list of datasets generated by *createFoldsAndNormalize()*, *AFT()* will take a single training set, testing set, and a specified distribution and learn the AFT model using the training set. Only one distribution can be tested at a time. In addition, the code contains a tryCatch command the catch the event where the AFT model fails to run, however, this only showed to occur for high dimensional datasets (when no feature selection was applied). 

As with the Kaplan-Meier model, the ouput is a list of three items, (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. 

**Example Usage**

```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
AFTMod = AFT(training1stFold, testing1stFold, "weibull")
curves = AFTMod[[1]]
curves
```

<br>

<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```
</center>


##Cox-Proportional Hazards with Kalbfleisch-Prentice Extension

The Cox proportional-hazards (Cox-PH) model is extremely common in the survival analysis literature. Previously, we mentioned the Kaplan-Meier model which modeled the survival function, \(S_{KM}(t)\) of a group of patients. Next, the Accelerated Failure Time model assumed a distribution on the survival time, \(T\), which allowed us to include features and build curves for individual patients. Instead, Cox-PH models the *hazard function* of patients, where hazard is interpretted as the risk of failure (dying) at any given time, specifically for a time \(t\) and covariates \(\boldsymbol{x}\) the hazard function is defined as
\[
h(t\,|\,\boldsymbol{x}) = \lim_{\Delta t \rightarrow \,0} \frac{P(t \leq T < t + \Delta t\,\,\, |\,\,\, T \geq t, \boldsymbol{x} )}{\Delta t}. 
\]

Cox-PH models this hazard function in terms of a baseline hazard, \(h_0(t)\), equal for all patients and scaled by a learned, individual risk dependening on features, that is, \(h(t|\boldsymbol{x}) = h_0(t)\,e^{\boldsymbol{w}^T\boldsymbol{x}}\). This is why the model is termed the *proportional hazards* model, the hazard for one patient is proportional to the hazard of all other patients, by a scale depending on individual features.

To learn the values for \(\boldsymbol{w}^T\) one does not need to know the baseline hazard function. Suppose a single patient died at time \(t_j\). The probability that patient \(i\) was the one who died is given by
\begin{align*}
\frac{h(t_j\,|\,\boldsymbol{x_i})}{\sum_{k \in R(t_j)} h(t_j\,|\,\boldsymbol{x_k})} \quad  &= \quad \frac{h_0(t_j)\,e^{\boldsymbol{w}^T\,\boldsymbol{x_i}}}{\sum_{k \in R(t_j)} h_0(t_j)\,e^{\boldsymbol{w}^T\,\boldsymbol{x_k}}}, \\
&= \quad \frac{e^{\boldsymbol{w}^T\,\boldsymbol{x_i}}}{\sum_{k \in R(t_j)} e^{\boldsymbol{w}^T\,\boldsymbol{x_k}}},
\end{align*}
where \(R(t_j)\) is the set of patients still alive (at risk) at time \(t_j\). Thus the likelihood for \(\boldsymbol{w}^T\) is defined as
\[
L\left(\boldsymbol{w}^T\right) = \prod_j \frac{e^{\boldsymbol{w}^T\,\boldsymbol{x_{i\,(j\,)}}}}{\sum_{k \in R(t_j)} e^{\boldsymbol{w}^T\,\boldsymbol{x_k}}},
\]
where subscript \(i (j)\) is interpreted as it is patient \(i\) who died at time \(j\).

By this formulation of the likelihood equation, we see that feature weights are independent of time, *e.g* in a study of survival post surgery, lab test results immedietly following surgery have the same weight one day after surgery as they do a year from surgery. 

While we are able to estimate feature weights without specifying the baseline hazard function, we cannot determine the survival distribution. A number of methods exist for calculating the the baseline hazard and therefore the survival distribution including the Kalbfleisch-Prentice estimator which we use in this work. The thesis ["Baseline Survival Function Estimator under Proporitional Hazards Assumption"](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjploaonvTcAhWKJ3wKHblTAXoQFjAAegQIBBAC&url=http%3A%2F%2Fstat.nuk.edu.tw%2Fgraduate%2F96_paper%2FM0944411.pdf&usg=AOvVaw1LAbWJ2e1MQL2KIXBPqIZA) by Weng provides a nice disucssion of the Breslow and Kalbfleisch-Prentice estimators.


###Cox-Proportional Hazards with Kalbfleisch-Prentice Extension Code (CoxPH_KP.R)
```{r, echo=FALSE}
#Library Dependencies
#survival is needed to get survfit and the implimentation of the KP estimator.
library(survival)
#For EN-Cox
library(fastcox)
#For sindex
library(prodlim)

CoxPH_KP = function(training, testing,ElasticNet=F){
  if(ElasticNet){
    timeInd = which(names(training) == "time")
    deltaInd = which(names(training) == "delta")
    alpha = NULL
    lambda = NULL
    bestError = Inf
    for(i in c(0.01,.2,.4,.6,.8,1)){
      model =  cv.cocktail(as.matrix(training[,-c(timeInd, deltaInd)]),training[,timeInd], training[,deltaInd],alpha = i)
      modelBestLambdaIndex = which(model$lambda == model$lambda.min)
      modelError = model$cvm[modelBestLambdaIndex]
      if(modelError < bestError){
        alpha = i
        lambda = model$lambda.min
        bestError = modelError
      }
    }
    coxModel = cocktail(as.matrix(training[,-c(timeInd, deltaInd)]),training[,timeInd], training[,deltaInd],alpha = alpha,lambda = lambda)
    linearPredictionsTraining = predict(coxModel,as.matrix(training[,-c(timeInd, deltaInd)]),type = "link")
    linearPredictionsTesting = predict(coxModel,as.matrix(testing[,-c(timeInd, deltaInd)]),type = "link")
    survivalEstimate = KPEstimator(linearPredictionsTraining, training$time,training$delta)
    survCurvs = t(sapply(survivalEstimate[[2]], function(x) x^exp(linearPredictionsTesting)))
    survivalCurves = list(time = survivalEstimate[[1]], surv = survCurvs)
  }
  else{
    tryCatch({
      coxModel = coxph(Surv(time,delta)~., data = training,singular.ok = T)
      survivalCurves = survfit(coxModel, testing, type = "kalbfleisch-prentice")
    },
    error = function(e) {
      message(e)
      warning("Cox-PH failed to converge (likely due to singularity). Future runs have been eliminated for Cox.")
    })
    if(!exists("coxModel") | !exists("survivalCurves")){
      return(NA)
    }
  }
  if(0 %in% survivalCurves$time){
    timePoints = survivalCurves$time
    probabilities = survivalCurves$surv
  } else{
    timePoints = c(0,survivalCurves$time)
    probabilities = rbind(1,survivalCurves$surv)
  }
  curvesToReturn = cbind.data.frame(time = timePoints, probabilities)
  timesAndCensTest = cbind.data.frame(time = testing$time, delta = testing$delta)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))  
}

#not considering ties.
KPEstimator = function(lp,lpTime,censorStatus){
  indexToKeep = sindex(sort(lpTime), unique(sort(lpTime)))
  orderLPTime = order(lpTime)
  cumHaz = rev(cumsum(rev(exp(lp[orderLPTime]))))
  alpha = ((1-(exp(lp[orderLPTime])/cumHaz)))^exp(-lp[orderLPTime])
  survivalFunc = cumprod(alpha^censorStatus[orderLPTime])
  return(list(time = lpTime[orderLPTime][indexToKeep],surv = survivalFunc[indexToKeep]))
}
```

**File Purpose**: Apply the Cox Proportional-Hazards model to data that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/CoxPH_KP.R".

**Functions**:

1. *CoxPH_KP(training, testing, ElasticNet)*
2. *KPEstimator(lp, lpTime, censorStatus)* - Used to estimate the Kalbfleisch-Prentice estimator when doing elastic-net Cox.

**Arguments**:

* *training*: The training set.
* *testing*: The testing set.
* *ElasticNet*: A boolean specifying whether or not to
* *lp*: The linear predictions (\(\boldsymbol{w}^T\boldsymbol{x}\)) from the Cox-PH model for the training data.
* *lpTime*: The training dataset event times, *i.e.* **time**.
* *censorStatus*: The training dataset censor status, *i.e* **delta**.

**Packages Required**:

1. survival -- Needed for *coxph()* and *survfit()*.
2. fastcox -- See Elastic-Net Cox.
3. prodlim -- See Elastic-Net Cox.

**File Notes**:

The primary function for this file is *CoxPH_KP()*; *KPEstimator()* is only used for the elastic-net Cox model.

Given the list of datasets generated by *createFoldsAndNormalize()*, *CoxPH_KP()* will take a single training set, testing set, and a boolean specifying whether or not to use the elastic-net model. For this section we assume ElasticNet = FALSE, that is we are NOT using the elastic net formulation. The code uses a tryCatch function to avoid models which are failing to converge (this happened for high dimensional datasets). The Cox-PH model is fit using *coxph()* from the survival package and then survival curves are fit using *survfit()* and specifying the type to be the Kalbfleisch-Prentice estimator. Ties are accounted for using the default of *coxph()*, *i.e.* the Efron approximation.

As with preious models, the ouput is a list of three items, (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. 

**Example Usage**

```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
CoxMod = CoxPH_KP(training1stFold, testing1stFold, ElasticNet=F)
curves = CoxMod[[1]]
curves
```
<br>

<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```
</center>

Note the preceding Figure depicts proportional hazards -- all curves are the exact same shape but at different heights, *i.e.* they are proportional to one another. 

##Cox-Proportional Hazards with Kalbfleisch-Prentice Extension -- Elastic Net

We include the elastic net version of Cox-PH to include a regularized version of Cox-PH to compete with other models such as Random Survival Forests and Multi-Task Logistic Regression which have built in regularization. The notation and implementations of elastic net come from Yang and Zou's paper entitiled ["A Cocktail Algorithm for Solving The Elastic Net Penalized Cox's Regression in High Dimensions""](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwiG3rLYuvTcAhUBN30KHfIQDYUQFjAAegQIABAC&url=http%3A%2F%2Fusers.stat.umn.edu%2F~zouxx019%2FPapers%2Ffastcox.pdf&usg=AOvVaw21oG6xeyfTb7jaKFBkYJct). The elastic net penalty adds the following term to the likelihood equation,
\[
P_{\lambda, \alpha}(\boldsymbol{w}) = \sum_{j=1}^p \lambda\left(\alpha\,|w_j| + \frac{1}{2}\,(1-\alpha)\, w_j^2 \right),
\]
for \(p\) features where \(\lambda > 0\) and \(0 < \alpha \leq 1\). The first penalty term, \(|w_j|\) corresponds to the LASSO loss and is responsible for feature selection whereas the quadratic term is to control the size of feature weights. For example, \(\alpha = 1\) would correspond to just LASSO-Cox, whereas \(\alpha \approx 0\) would correspond to Ridge-Cox. 

For more information regarding elastic net Cox see the Yang and Zou's paper referenced above.

###Cox-Proportional Hazards with Kalbfleisch-Prentice Extension -- Elastic Net Code (CoxPH_KP.R)

**File Purpose**: Apply the Cox Proportional-Hazards model to data that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/CoxPH_KP.R".

**Functions**:

1. *CoxPH_KP(training, testing, ElasticNet)*
2. *KPEstimator(lp, lpTime, censorStatus)* - Used to estimate the Kalbfleisch-Prentice estimator when doing elastic net Cox.

**Arguments**:

* *training*: The training set.
* *testing*: The testing set.
* *ElasticNet*: A boolean specifying whether or not to
* *lp*: The linear predictions (\(\boldsymbol{w}^T\boldsymbol{x}\)) from the Cox-PH model for the training data.
* *lpTime*: The training dataset event times, *i.e.* **time**.
* *censorStatus*: The training dataset censor status, *i.e* **delta**.

**Packages Required**:

1. survival -- Needed for the normal Cox implementation.
2. fastcox -- Used for the implementation of elastic net cox (*cocktail()*)
3. prodlim -- Used for the *sindex()* function -- sindex returns the indexs of positions, ideal for evaluating step functions.

**File Notes**:

Given the list of datasets generated by *createFoldsAndNormalize()*, *CoxPH_KP()* will take a single training set, testing set, and a boolean specifying whether or not to use the elastic-net model. For this section we assume ElasticNet = TRUE, that is we are using the elastic net formulation. There are two hyperparameters involved for elastic net Cox, the strength of the penalty, \(\lambda\), and the type of penalty (more LASSO like or more Ridge like), \(\alpha\). Six values are tested for \(\alpha\), 0.01, 0.2, 0.4, 0.6, 0.8, and 1.0. For each of these values the function *cv.cocktail()* tests 100 values of \(\lambda\) using 5-fold cross validation and returns \(\lambda_{min}\), the optimal value of \(\lambda\) to minimize the cross validation error (see the *cocktail()* function of the [package documentation](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=2ahUKEwjCmMChxvTcAhUMIjQIHTumDoEQFjABegQICRAC&url=https%3A%2F%2Fcran.r-project.org%2Fweb%2Fpackages%2Ffastcox%2Ffastcox.pdf&usg=AOvVaw18r4av9gbxAKwh-LuEP3j5)).The optimal value for \(\alpha\) and \(\lambda\) from interval 5-fold cross validation are then used for evaluting the test fold. Note the optimal values for \(\alpha\) and \(\lambda\) will (likely) differ across folds.

One the feature weights have been estimated, linear predictors are passed to *KPEstimator()* to estimate the survival curves. As of now, ties are not accounted for and the resulting survival distribution are using the estimate derived in the pressence of no ties. THIS NEEDS TO BE UPDATED TO EITHER USING THE BRESLOW ESTIMATOR OR NUMERICALLY GETTING ESTIMATING SURVIVAL CURVES IN THE PRESENCE OF TIES.

As with preious models, the ouput is a list of three items, (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. 

**Example Usage**

```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
CoxMod = CoxPH_KP(training1stFold, testing1stFold, ElasticNet=T)
curves = CoxMod[[1]]
curves
```
<br>

<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```
</center>



##Random Survival Forest - Kaplan-Meier Extension 

[Random Survival Forests](https://arxiv.org/pdf/0811.1645) (RSFs) were developed by Ishwaran et al. in 2008 and have been growing in popularity in recent years. While we do not give vast detail regarding the formation of these forests, the [original paper](https://arxiv.org/pdf/0811.1645) and [R package details](https://kogalur.github.io/randomForestSRC/theory.html) provide an excellent overview. 

The primary focus for RSFs were to build a risk score using an averaged cumulative hazard from the terminal nodes which in which patients landed. As previously mentioned here and explained further in our paper, risk scores are useful for discriminating between patients, but do not provide individual survival distributions.While survival distributions are not mentioned in the original RSF paper, the R package details briefly describe how RSFs build individual survival distributions in Section 8.1. Specifically, they state survival estimates are computed using Kaplan-Meier estimators. However, given a total of \(M\) trees in a forest, there will be \(M\) Kaplan-Meier estimators for each patient and the details of how these Kaplan-Meier estimators were combined are not given. After some backwards engineering I was able to come up with the conclusion that Kaplan-Meier curves are "averaged" across all the Kaplan-Meier curves - see Figure below.


<center>
```{r, echo=FALSE}
time1 = c(3,5,8,10,15)
cens1 = c(1,1,1,1,1)
time2 = c(6,9,10,17,20)
cens2 = c(1,1,1,1,1)
dat = cbind.data.frame(time1,cens1,time2,cens2)


p1 = prodlim(Surv(time1,cens1)~1,data = dat)
p2 = prodlim(Surv(time2,cens2)~1, data = dat)
KM1 = predict(p1, seq(0,20,by = 0.01)) 
KM2 = predict(p2, seq(0,20,by = 0.01))
KM1 = ifelse(is.na(KM1),0,KM1)
KM2 = ifelse(is.na(KM2),0,KM2)
RKM = (KM1 + KM2)/2

dat2 = cbind.data.frame(time = seq(0,20,by=0.01), RKM = RKM, KM1 = KM1, KM2 = KM2)

ggplot(data = dat2)+
  geom_line(aes(x = time, y = KM1), color = "dodgerblue2",size = .8)+
  geom_line(aes(x = time, y = KM2), color = "dodgerblue2", size = .8)+
  geom_line(aes(x = time, y = RKM), color = "orangered2",size = 1) +
  theme_bw()+
  labs(x = "Time",y = "Survival Probability")+
  theme(text = element_text(size = 18, face = "bold"))

```
</center>

<figcaption>Figure: Depiction of averaing two Kaplan-Meier curves together to produce one individual survival distribution for a test patient. Blue Kaplan-Meier curves represent the Kaplan-Meier estimators in two terminal nodes whereas the red curve represents the combination - giving form to the individual survial distribution.</figcaption>

###Random Survival Forest - Kaplan-Meier Extension Code (RandomSurvivalForests.R)
```{r, include = FALSE}
library(survival)
library(randomForestSRC)

RSF = function(training, testing, ntree){
  rsfMod = rfsrc(Surv(time,delta)~., data = training, ntree = ntree)
  survivalCurves = predict(rsfMod, testing)
  trainingTimes = survivalCurves$time.interest
  if(0 %in% trainingTimes){
    times = trainingTimes
    probabilities = t(survivalCurves$survival)
  } else{
    times = c(0,trainingTimes)
    probabilities = rbind.data.frame(1,t(survivalCurves$survival))
  }
  curvesToReturn = cbind.data.frame(time = times, probabilities)
  timesAndCensTest = cbind.data.frame(time = testing$time, delta = testing$delta)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))  
}
```


**File Purpose**: Apply the RSF model to data that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/RandomSurvivalForests.R".

**Functions**:

1. *RSF(training, testing, ntree)*

**Arguments**:

* *training*: The training set.
* *testing*: The testing set.
* *ntree*: The number of trees.

**Packages Required**:

1. survival -- Needed for the *Surv()* function
2. randomForestSRC -- Used for the RSF implementation.

**File Notes**:

Given the list of datasets generated by *createFoldsAndNormalize()*, *RSF()* will take a single training set, testing set, and an inteer specifying the number of trees to use in the forest and build the RSF model using *rfsrc()* from the randomForestSRC package. Then the *predict()* functon for rfsrc is used to build survival curves for test patients.

As with previous models, the ouput is a list of three items, (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. 

**Example Usage**

```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
rsfMod = RSF(training1stFold, testing1stFold, ntree = 1000)
curves = rsfMod[[1]]
names(curves) = c("time",1:26)
curves
```
<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```
</center>


##Multi-task Logistic Regression (MTLR.R)

Of the models tested, Multi-task Logistic Regression (MTLR) is the least well known so we give a brief description here. Consider modeling the probability of survival of patients at each of a vector of time points
$\tau = [t_1, t_2, \ldots, t_m]$, *e.g.* $\tau$ could be the 60 monthly intervals from 1 month up to 60 months. 
We can set up a series of logistic regression models:
For each patient, representated as $\vec{x}$,
\[
P(T \geq t_i \,|\, \vec{x}) =  \left(1 + \exp(\vec{\theta_{i}}\cdot \vec{x} )\right)^{-1}
\]
where $\vec{\theta_i}$  are the time-specific feature vectors.
While the input features $\vec{x}$ stay the same for all these classification tasks,  
the binary labels $y_i = [T\geq t_i]$ can change depending on the threshold $t_i$.%

We encode the survival time $d$ of a patient as a binary sequence 
$y = y(d) = (y_1, y_2, \ldots, y_m)$, where $y_i = y_i(d) \in \{0,1\}$ denotes the survival status of the patient at time $t_i$, 
so that $y_i = 0$ (no death event yet) for all $i$ with $t_i <d$, and $y_i = 1$ (death) for all $i$ with $t_i \geq d$.



Here there are $m+1$ possible legal sequences of the form $(0,0,\ldots,1,1,\ldots,1)$, including the sequence of all 0's and the sequence of all 1's. 
The probability of observing the survival status sequence $y = (y_1, y_2, \ldots, y_m)$ can be represented as:
\[
  P_\Theta(Y\!\! =\!\! (y_1, y_2, \ldots, y_m) \,\,|\,\, \vec{x}) = \frac{\exp(\sum_{i=1}^m y_i \times \theta_{i}\cdot\vec{x} )}
     {\sum_{k=0}^m \exp(f_{\Theta}(\vec{x}, k))}, 
\]
where $\Theta = (\theta_{1}, \ldots, \theta_{m})$, and
$f_{\Theta}(\vec{x}, k) = \sum_{i=k+1}^m (\theta_{i}\cdot\vec{x})$ 
for $0\leq k\leq m$ is the score of the sequence with the event occurring in the interval $[t_k, t_{k+1})$ before taking the logistic transform, with the boundary case $f_{\Theta}(\vec{x}, k)= 0$ being the score for the sequence of all `0's. 
Given a dataset of $n$ patients $\{ \vec{x_r}\}$ with associated time of deaths $\{ d_r \}$, 
we find the optimal parameters (for the \MTLR\ model) $\Theta^*$ as
\[
\Theta^*\ =\
\arg\max_{\Theta} 
\sum_{r=1}^n \left[\sum_{i=1}^m y_j(d_r)(\theta{i}\!\cdot\!\vec{x_r})\! - \log \sum_{k=0}^m \exp f_{\Theta}(\vec{x_r},k) \right]
  -  
\frac{C}{2}\!\sum_{j=1}^m\! \|\theta_{j}\|^2\! 
% +\! \frac{C_2}{2}\sum_{j=1}^{m-1}\! \|\theta_{j+1}\!-\!\theta_j\|^2\!
\]
where the $C$ (for the regularizer) is found by an internal cross-validation process.
}


There are many details here -- \eg 
to insure that the survival function starts at 1.0, and decreases monotonically and smoothly until reaching 0.0 for the final time point; 
to deal appropriately with censored patients; 
to decide how many time points to consider ($m$); and 
to minimize the risk of overfitting (by regularizing),
and by selecting the relevant features.
The [paper by Yu et al.](https://papers.nips.cc/paper/4210-learning-patient-specific-cancer-survival-distributions-as-a-sequence-of-dependent-regressors) provides the details.


###Multi-task Logistic Regression Code (MTLR.R)
```{r, include = FALSE}
MTLR = function(training, testing,linearTail=T){
  #The idea here is to have the working directory sitting in ISDEvaluationPipeline. We move the working directory into the folder
  #with executables for ease of execution and move back to the original working directory before exiting the function.
  executablesPath = "Models/AdditionalMTLRFiles/"
  originalWd = getwd()
  setwd(paste(originalWd,"/",executablesPath,sep=""))
  #Write csv files to be called by CovertDataFiles to make the correct format of input for MTLR.
  write.csv(training, paste("training.csv",sep=""), row.names = F)
  write.csv(testing, paste("testing.csv",sep=""), row.names = F)
  system("java -cp ./ ConvertDataFiles convert2MTLR training.csv training.mtlr FlipCensoredBit")
  system("java -cp ./ ConvertDataFiles convert2MTLR testing.csv testing.mtlr FlipCensoredBit")
  system("./mtlr_opt -i training.mtlr",ignore.stdout = T)
  system("./mtlr_test -i testing.mtlr -s training.mtlr -o ./fold1_modelfile > MTLR_output.txt")
  times = unlist((unname(read.table("fold1_modelfile",skip = 1,sep = ",",nrows = 1))))
  if(!linearTail){
    #It appears the last survival probability is always appearing to be zero... which is odd.
    #There are more survival probabilities than survival time points. Further, the previous writer of code (Fatima) used the last time point
    #squared divided by the second to last time point. I'm guessing that that this was done using some information not known to me at 
    #this time so until further notice we will do the same. 
    lastTimePoint = round(times[length(times)]^2/times[length(times) -1])
    #Add the last time point and a 0 time point if needed.
    if(0 %in% times){
      timePoints = c(times,lastTimePoint)
    } else{
      timePoints = c(0,times, lastTimePoint)
    } 
  }
  else{
    if(0 %in% times){
      timePoints = times
    } else{
      timePoints = c(0,times)
    } 
  }

  testingPoints = read.table("MTLR_output.txt")
  #Clean up directory:
  system("rm fold1_modelfile CI_log Ptrain1 Pmodel1 *.csv *.mtlr *.txt")
  #Replace original working directory.
  setwd(originalWd)
  #the first 4 columns are the true time of death, 1- censoring status, and 2 different averaged survival times.
  trueDeathTimes = testingPoints[,1]
  censorStatus = 1 -testingPoints[,2]
  #We don't want the averages (first 3 columns) and the last column is some evaluation of survival probability at the true time of death.
  #We will discard this since later on we have a method used for every survival curve. Further we minus 1 because we dont want to include the 
  #0th time point, i.e. since we use length(timePoints) we need to subtract an extra value it since we included a 0.
  #So we have ignore first 4 columns through the number of time points, subtracting 1 for the 0th time point (if we included a zero).
  if(0 %in% times){
      survivalProbabilities = testingPoints[5:(4+length(timePoints))]
  } else{
      survivalProbabilities = testingPoints[5:(4+length(timePoints)-1)]
    }
  #Survival probabilities were read in a factors and contain commas. We will clean this out by turning them into character vectors and 
  #trimming commas.
  survivalProbabilities = apply(survivalProbabilities,c(1,2),as.character)
  survivalProbabilities = apply(survivalProbabilities,c(1,2), function(x) as.numeric(gsub(",","",x)))
  #Some of the last survival probabilities were negative so we turned these to zero. There were values like -1.2239e-17 so effectively 
  #zero anyways. Additionally we need to transpose the survival probabilities to match up with the survival time estimates and then
  #add a survival probability of 1 to the 0th time point if there was no original 0 time point.
  if(0 %in% times){
    survivalProbabilities = t(apply(survivalProbabilities, c(1,2), function(x) ifelse(x < 0,0,x)))
  } else {
    survivalProbabilities = rbind(1,t(apply(survivalProbabilities, c(1,2), function(x) ifelse(x < 0,0,x))))
  }
  curvesToReturn = cbind.data.frame(time = timePoints, survivalProbabilities) 
  timesAndCensTest = cbind.data.frame(time = trueDeathTimes, delta = censorStatus)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))  
}
```

```{r, warning=FALSE}
setwd("./ISDEvaluationPipeline")
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
mtlrMod = MTLR(training1stFold, testing1stFold)
curves = mtlrMod[[1]]
row.names(curves) = NULL
curves
```
<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```

#Evaluting ISD Models

##Concordance 

###Concordance Code (Concordance.R)

##1-Calibration 

###1-Calibration Code (OneCalibration.R)

##Brier Score

###Brier Score Code (BrierScore.R)

##L1-Loss 

###L1-Loss Code (L1Measures.R) 
 
##D-Calibration (DCalibration.R)

###D-Calibration Code (DCalibration.R)


#Miscellanious

##Plotting Individual Survival Curves (plotSurvivalCurves.R)

##The Code - From Beginning To End 
























