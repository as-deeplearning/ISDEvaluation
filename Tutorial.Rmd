---
title: "Individual Survival Distributions - Case Study and Code Tutorial"
author: "Humza Haider"
date: "`r format(Sys.time(), '%B %e, %Y')`"
output:
  html_notebook: 
    toc: TRUE
    number_sections: true
    fig_caption: yes
    fig_align: "center"
---


This article will serve as a tutorial for the code which accompanies the paper "Effective Ways to Build and Evaluate Individual Survival Distributions" by Haider et al. The goal here is not only to show how one may make their own experiments but also address specific implementation details which would have been too cumbersome to include in the original paper. The outline of this tutorial is as follows: COME UP WITH SOMETHING.


#Introduction to the Example Data

The code here has been written for *survival* datasets, *i.e.* datasets are comprised of (1) a variable which indicates time of event, (2) an indicator variable for RIGHT censoring, and (3) features for each patient. Note that the code only allows for right censored patients, left censored or interval censored data will not work with the current implementation. 

As a case study we will use the Northern Alberta Cancer Dataset (NACD) which was also included in the original paper. However, to spice things up a bit we will use a new subset of the dataset, namely we will only consider patients with stage 4 stomach cancers. This data contains a total of 128 patients (100 uncensored) and 53 features. Below we give the summary statistics of the first five variables:

```{r}
NACD = read.csv("NACDS4SRAW.csv")
head(NACD)
summary(NACD[,1:5])
```


Here, we have "Time" as the variable indicating the time until the event occured and "Status" representing the indicator for death, that is, a 1 indicates the patient died and a 0 indicates the patient was right censored. To examine the other 48 features of the dataset in detail feel free to download the data which is freely avaliable (somewheres?). 

#Preproccesing the Data

##Validating and Cleaning  (validateAndClean.R)

**File Purpose**: This file is used to make sure the survival dataset is in the correct format for analysis. For example, the variable used for survival time must be named exactly **time**. Additionally, a minimal amount of data cleansing is completed in this file.

**File Path**: "ISDEvaluationPipeline/ValidateCleanCV/validateAndClean.R".

**Functions**:

1. *validateAndClean(survivalDataset, imputeZero=T)*
2. *validate(survivalDataset, imputeZero)*
3. *clean(survivalDataset)*

Note: You will notice this file structure throughout the codebase, there will often be a master function (here *validateAndClean()*) which runs the other functions included in the file, *e.g.* *validate()* and *clean()*. 

**Arguments**:

* *survivalDataset*: a (non-empty) dataframe representing the survival dataset -- *e.g.* NACD
* *imputeZero*: a boolean indicating if zero valued survival times should be imputed (see below for details).

**Packages Required**:

1. caret -- The *nearZeroVar()* function to identify variables with only 1 unique value and remove them in the *clean()* function.


###*validate(survivalDataset, imputeZero)*

The function *validate()* checks a number of things:

```{r, echo=FALSE }
validate = function(survivalDataset,imputeZero){
  if(is.null(survivalDataset) || nrow(survivalDataset) == 0)
    stop("You must enter a nonempty survival dataset.\n")
  if(!"time" %in% names(survivalDataset))
    stop("The variable 'time' in not included in the given dataset.\n")
  if(!"delta" %in% names(survivalDataset))
    stop("The variable 'delta' in not included in the given dataset.\n")
  survivalDataset$time = as.numeric(survivalDataset$time)
  survivalDataset$delta = as.numeric(survivalDataset$delta)
  #We want to change the ordering so that the time column is first and the delta column is second.
  timeIndex = which(names(survivalDataset) == "time")
  deltaIndex = which(names(survivalDataset) == "delta")
  survivalDataset = survivalDataset[,c(timeIndex,deltaIndex, (1:ncol(survivalDataset))[-c(timeIndex,deltaIndex)])]
  if(any(is.na(survivalDataset$time))){
    warning("'time' includes NA values. These rows have been removed.\n")
    NAtimeIndex = which(is.na(survivalDataset$time))
    survivalDataset = survivalDataset[-NAtimeIndex,]
  }
  if(any(is.infinite(survivalDataset$time))){
    warning("'time' includes Inf values. These rows have been removed.\n")
    InftimeIndex = which(is.infinite(survivalDataset$time))
    survivalDataset = survivalDataset[-InftimeIndex,]
  }
  if(any(survivalDataset$time <0)){
    warning("'time' includes negative values. These rows have been removed.\n")
    nonPosTimeIndex = which(survivalDataset$time <0)
    survivalDataset = survivalDataset[-nonPosTimeIndex,]
  }
  if(any(survivalDataset$time ==0) & imputeZero){
    imputeVal  =min(survivalDataset$time[survivalDataset$time > 0])/2
    warning(paste("'time' includes 0 valued times. These have been imputed to half the minimum positive time point:",
                  imputeVal,"\n"))
    zeroTimeIndex = which(survivalDataset$time ==0)
    survivalDataset[zeroTimeIndex,]$time = imputeVal
  }
  if(any(is.na(survivalDataset$delta))){
    warning("'delta' includes NA values. These rows have been removed.\n")
    NAdeltaIndex = which(is.na(survivalDataset$delta))
    survivalDataset = survivalDataset[-NAdeltaIndex,]
  }
  if(any(is.infinite(survivalDataset$delta))){
    warning("'delta' includes Inf values. These rows have been removed.\n")
    InfdeltaIndex = which(is.infinite(survivalDataset$delta))
    survivalDataset = survivalDataset[-InfdeltaIndex,]
  }
  if(length(unique(survivalDataset$delta)) >2)
    stop("'delta' contains more than two unique values. 'delta' should only contain 0 and 1.\n")
  if(!all(sort(unique(survivalDataset$delta)) == c(0,1)))
    stop("'delta' contains values other than 0 and 1. 0 should indicate RIGHT censoring and 1 should indicate the event occuring.\n")
  return(survivalDataset)
}
```

1. A survival dataset was nonempty.
2. The survival time variable MUST be named **time**.
3. Similarly, the indicator of death variable must be named **delta**. The naming choice of this variable is the common notation in survival analysis.
4. If **time** includes NA values these rows will be removed.
5. If **time** includes Inf values these rows will be removed.
6. If **time** includes negative values these rows will be removed.
7. If **time** includes zero values there are two options depending on the value of the argument imputeZero. If imputeZero = TRUE, then zero valued times will be imputed to half the minimum non zero valued time. For example in the NACD dataset we are using here the minimum time is 0.2333. If there was another row where the event time was 0, this would be imputed to $\frac{0.2333}{2} = 0.1166$. We include this option as some survival models (*e.g.* Accelerated Failure Time with the Weibull distribution) will fail if given a zero valued time. If imputeZero = FALSE then nothing will happen to these zero valued survival times. Currently there is no option to remove zero valued times other than removing them from the dataset prior to running the code avalibale here.
8. If **delta** includes NA values these rows will be removed.
9. If **delta** includes Inf values these rows will be removed.
10. **delta** must only have two unique values.
11. The unique values of **delta** must be 0 and 1 -- 0 for censored data and 1 for uncensored data.

If we try to use *validate* on our NACD dataset we will get an error since currently our survival time variable is named "Time" and our death indicator is named "Status". Note that below we include imputeZero = TRUE, but since there are no zero-valued times the same result would occur if we included imputeZero=FALSE.

```{r, error= TRUE}
validatedNACD = validate(NACD, imputeZero = TRUE)
names(NACD)[1] = "time"
validatedNACD = validate(NACD, imputeZero = TRUE)
names(NACD)[2] = "delta"
validatedNACD = validate(NACD, imputeZero = TRUE)
```

###*clean(survivalDataset)*

```{r, echo = FALSE,message=FALSE}
library(caret)
clean = function(survivalDataset){
  #Remove columns with no variance.
  allSame = nearZeroVar(survivalDataset,freqCut = 100/0)
  if(length(allSame)>0){
    namesToRemoveNoVar = names(survivalDataset)[allSame]
    survivalDataset = survivalDataset[,-allSame]
    warning(paste("The variable(s)",paste(namesToRemoveNoVar, collapse = ", "),"contained only 1 unique value. They have been removed.\n",
                  sep = " "))
  }
  #Remove columns with over 25% of data missing.
  naProp = apply(survivalDataset, 2, function(x) sum(is.na(x))/nrow(survivalDataset))
  toRemoveNA = which(naProp > 0.25)
  if(length(toRemoveNA)>0){
    namesToRemoveNA = names(survivalDataset)[toRemoveNA]
    survivalDataset = survivalDataset[,-toRemoveNA]
    warning(paste("The variable(s)",paste(namesToRemoveNA, collapse = ", "),"contained over 25% NA values. They have been removed.\n",
                  sep = " "))
  }
  indicatorVariables = sapply(survivalDataset[,-c(1,2)], function(x) all(unique(x)[!is.na(unique(x))] %in% c(0,1)) || all(unique(x)[!is.na(unique(x))] %in% c(-1,0)))
  if(any(indicatorVariables)){
  survivalDataset[,-c(1,2)][,indicatorVariables] = lapply(survivalDataset[,-c(1,2)][,indicatorVariables], factor)
  warning(paste(toString(names(survivalDataset[-c(1,2)][indicatorVariables])), "had only unique values c(0,1) or c(-1,0). These were converted to factors.",sep = " "))
  }
  return(survivalDataset)
}
```


Here, *clean()* does a very minimal amount of cleaning on the survival dataset. Specifically, the goal is to remove variables which will be not be meaningful to analysis. There are two cases where variables are removed:

1. A variable contains all unique values. For example, in our NACD dataset there is a **STAGE_4** variable which is an indicator (1 or 0) whether or not the cancer is in stage 4. Since we only have stage 4 stomach cancer patients this variable will be be all 1s and will be removed from the dataset.
2. Variables such that 25% of rows (patients) have a missing value will be removed. However, the coding of missing data must be such that is.na() would identify the data as missing. For example is.na("") returns TRUE even though an empty string likely indicates a missing value.
3. Often if a factor (nominal) variable only contains two values (say Male and Female) individuals will not change this to explicitly be a factor and leave the variable in terms of 1s and 0s. However, if these variables also contain missing values this will create weird behaviors when doing imputations, e.g. a missing value would be imputed as the average gender *e.g* 0.7 which is meaningless. For this reason we MAKE THE ASSUMPTION that all variables which only contain non-NA unique values of c(0,1) or c(0,-1) are actually factor variables and this change is made to the data.

Whenever a variable is removed or turned into a factor, *clean()* will return a warning indicating the name of the variable(s) which were removed/factored. For example, on our validated NACD dataset we can now call *clean()*:


```{r}
cleanedNACD = clean(validatedNACD)
```
As noted above a number of variables were removed due to having only 1 unique value. 


###*validateAndClean(survivalDataset, imputeZero=T)*

```{r, echo = FALSE,message=FALSE}
validateAndClean = function(survivalDataset, imputeZero=T){
  validatedData = validate(survivalDataset, imputeZero)
  cleanData = clean(validatedData)
  return(cleanData)
}
```


This function simply runs both *validate()* and *clean()* on a survivalDataset. Note that here we default imputeZero to be TRUE. Below we verify that *validateAndClean(NACD)* is equivalent to cleanedNACD.

```{r, warning=FALSE}
all(validateAndClean(NACD) == cleanedNACD)
```


##Creating Cross-Validation Folds and Normalizing Features (createFoldsAndNormalize.R)

**File Purpose**: This file is used to create folds from the survival dataset for cross validation and normalize features. As part of normalization, mean/mode imputation is employed to handle missing data.

**File Path**: "ISDEvaluationPipeline/ValidateCleanCV/createFoldsAndNormalize.R".

**Functions**:

1. *createFoldsAndNormalize(survivalDataset, numberOfFolds)*
2. *createFoldsOfData(survivalDataset, numberOfFolds)*
3. *meanImputation(listOfDatasets)*
4. *normalizeVariables(listOfImputedDatasets)*


**Arguments**:

* *survivalDataset*: the survivalDataset AFTER going through validation and cleaning -- *e.g.* cleanedNACD
* *numberOfFolds*: the desired number of cross-validation folds, must be greater than 1.
* *listOfDatasets*: a list containing a list of Training datasets and a list of Testing datasets - *i.e.* the output of *createFoldsOfData()*
* *listOfImputedDatasets*: same as *listOfDatasets* but missing values have been imputed (the output of *meanImputation()*)

**Packages Required**:

1. caret -- The *dummyVars()* function is used to create a one hot encoding of factor (nominal) variables.
2. dataPreparation -- The *build_scales()* and *fastScale()* function are used to normalize variables.

###*createFoldsOfData(survivalDataset, numberOfFolds)*{#folds}
```{r, echo=FALSE}
createFoldsOfData = function(survivalDataset, numberOfFolds){
  #Create folds with the equal amounts of censoring and quasi equal ranges. Here we order by censor status and time to event
  #and then match these off to folds one by one. Doing it this way makes cross validation deterministic which is not ideal
  #but lets us have equally matched folds.
  time = survivalDataset$time
  delta = survivalDataset$delta
  Order= order(delta,time)
  foldIndex = lapply(1:numberOfFolds, function(x) Order[seq(x,nrow(survivalDataset), by = numberOfFolds)])
  listOfTestingSets = lapply(foldIndex, function(indexs) survivalDataset[indexs,])
  listOfTrainingSets = lapply(foldIndex, function(indexs) survivalDataset[-indexs,])
  listOfDatasets = list(Training = listOfTrainingSets, Testing = listOfTestingSets)
  return(list(foldIndex,listOfDatasets))
}
```
This code base requires that data be in cross-validation format with a minimum of 2 folds. The cross-validation used here is specific to survival analysis in that cross-validation is deterministic -- rows are first stratefied by their censoring status (**delta**) and then sorted by **time** and sequentially placed into folds (see Figure 1). The reasoning behind this procedure is two-fold (pun-intended):

1. The number of censored patients should be (roughly) equal across all folds.
2. The range of event times should also be (roughly) equal across all folds.

By creating folds in this way models will be tested on data which will be mostly representitive of what they saw in the training data.

![Figure 1: Depiction of 3-fold cross-validation scheme used for survival data. The dataset is first stratefied by delta and then sorted by time. Rows are then sequentially ordered into the three folds.](./SurvivalCV.png)

In addition to creating cross validation folds, *createFoldsOfData()* also returns the indexs of the rows that each test set belongs. See the below example for details. Here we put our cleaned NACD dataset (cleanedNACD) into 5 folds:

```{r}
folds = createFoldsOfData(cleanedNACD, 5)
#folds comprises a list of (2) items. (1) is the indexs of the test set. For example, examine the first test set fold:
testSetIndexs = folds[[1]]
#Test fold 1:
testSetIndexs[[1]]
```
Here the first test fold is comprised of the 123rd, 101st, 62nd, ..., and the 77th rows of the original data. We will later use these indexs for plotting in Section ??????.

The second item returned by *createFoldsOfData()* is a list of (1) the training sets, and (2) the testing sets. Below we simply give the dimension of each:
```{r}
trainingAndTestingSets = folds[[2]]
#First Training Set:
dim(trainingAndTestingSets$Training[[1]])
#First Testing Set:
dim(trainingAndTestingSets$Testing[[1]])
```

###*meanImputation(listOfDatasets)*

```{r, include = FALSE}
meanImputation = function(listOfDatasets){
  #Here we take the means (numeric variables) and modes (factor variables) of the training and data and impute the training AND the 
  #testing data with the same mean/mode of the respective training data.
  for(i in 1:length(listOfDatasets$Training)){
    train = listOfDatasets$Training[[i]]
    test = listOfDatasets$Testing[[i]]
    #Note that we are also imputing time and delta, but validation removed all NA instances of time and delta so we are really imputing 
    #nothing.
    
    #For numeric variables:
    trainNumeric = train[,sapply(train,is.numeric), drop = FALSE]
    #drop = FALSE makes it stay as a dataframe instead of turning to a vector in the event there is only 1 variable with the condition.
    testNumeric = test[,sapply(test,is.numeric), drop = FALSE]
    varMeans = apply(trainNumeric,2,function(x) mean(x, na.rm = T))
    trainNumericImputed = apply(trainNumeric, 2, function(x) ifelse(is.na(x),mean(x, na.rm = T),x))
    #The function is more complex for test because we have to specify the means and sd of the TRAINING set as we go through 
    #the columns of the test set.
    testNumericImputed = as.data.frame(sapply(1:length(varMeans), function(x) ifelse(is.na(testNumeric[,x]),varMeans[x],testNumeric[,x])))
    #Names are getting lost in sapply
    names(testNumericImputed) = names(testNumeric)
    
    
    #For factor variables:
    trainFactor = train[,sapply(train,is.factor),drop=FALSE]
    testFactor = test[,sapply(test,is.factor),drop=FALSE]    
    if(ncol(trainFactor) > 0){
      #We need to introduce a mode function to find the mode of the factor variables.
      Mode <- function(x) {
        #Modified from https://stackoverflow.com/questions/2547402/is-there-a-built-in-function-for-finding-the-mode
        x = x[!is.na(x)]
        ux <- unique(x)
        ux[which.max(tabulate(match(x, ux)))]
      }
      varModes = apply(trainFactor,2,function(x) Mode(x))
      trainFactorListed = lapply(trainFactor, as.character)
      trainFactorImputed = as.data.frame(lapply(trainFactorListed, function(x) ifelse(is.na(x), Mode(x), x)))
      #We run into problems if the training dataset didn't have a factor level in it which was included in the 
      #original dataset. Here we are just adding back the original factor levels.
      for(j in 1:ncol(trainFactorImputed)){
        missedLevels = levels(trainFactor[,j])[which(!levels(trainFactor[,j]) %in% levels(trainFactorImputed[,j]))]
        levels(trainFactorImputed[,j]) = c(levels(trainFactorImputed[,j]), missedLevels)
      }
      testFactorImputed = as.data.frame(sapply(1:length(varModes),
                                               function(x) factor(ifelse(is.na(testFactor[,x]),
                                                                         varModes[x],
                                                                         paste(testFactor[,x])),
                                                                  levels = levels(trainFactorImputed[,x]))))
      names(testFactorImputed) = names(testFactor)
      #Combine numeric and factor variables and save over the previous datasets.
      listOfDatasets$Training[[i]] = cbind.data.frame(trainNumericImputed, trainFactorImputed)
      listOfDatasets$Testing[[i]] = cbind.data.frame(testNumericImputed, testFactorImputed)
    }
   else{
     listOfDatasets$Training[[i]] = as.data.frame(trainNumericImputed)
     listOfDatasets$Testing[[i]] = as.data.frame(testNumericImputed)
   }
  }
  return(listOfDatasets)
}
```

Following the splitting of folds we perform imputation for missing data. Since the primary focus of this work was not in any way geared towards imputation methods we do a very simple mean/mode imputation for missing values WITHIN FOLD. The means for all numerical variables for each training fold are computed and the corresponding test fold is imputed with the mean value from the training fold. Similarly, factor variables are imputed with the mode factor level. Note that by either imputing missing values for the entire dataset or imputing training and test folds seperatly would lead data leakage. Note that **time** and **delta** are not subject to imputation -- if either of these variables contained missing values the entire row was removed in *validate()*.

Below we would impute missing data, however, the NACD dataset we are using has no missing data so the following step is not necasarry.
```{r, echo=TRUE,include=TRUE}
imputedData = meanImputation(trainingAndTestingSets)
```

###*normalizeVariables(listOfImputedDatasets)*
```{r, include=FALSE}
library(dataPreparation)
normalizeVariables = function(listOfImputedDatasets){
  listOfNormalizedDatasets = list()
  for(i in 1:length(listOfImputedDatasets$Training)){
    train = listOfImputedDatasets$Training[[i]]
    test = listOfImputedDatasets$Testing[[i]]
    #We need to remove time an delta so they don't get normalized. Here we set them apart and make an indicator for their location.
    timeDeltaInd = which(names(train) %in% c("time","delta"))
    timeDeltaTrain = train[,timeDeltaInd]
    timeDeltaTest = test[,timeDeltaInd]
    
    #fullRank =T drops one of the extra columns for the one-hot encoding.
    oneHotEncoder = dummyVars("~.",data = train[-timeDeltaInd], fullRank = T)
    trainEncoded = predict(oneHotEncoder, train[-timeDeltaInd])
    testEncoded = predict(oneHotEncoder, test[-timeDeltaInd])
    
    scales = build_scales(trainEncoded, verbose = F)
    trainCentered = cbind.data.frame(timeDeltaTrain,fastScale(trainEncoded, scales=scales, verbose=F))
    testCentered = cbind.data.frame(timeDeltaTest,fastScale(testEncoded, scales=scales,verbose=F))
    
    #If a variable had zero variance in the test set we will end up dividing by 0 and getting NaN values.
    #Here we simply make everything 0, the mean value, if this occurs.
    trainCentered = apply(trainCentered,2, function(x){
      if(all(is.nan(x))) 
        return(rep(0,length(x)))
      return(x)
    })
    testCentered = apply(testCentered,2, function(x){
      if(all(is.nan(x))) 
        return(rep(0,length(x)))
      return(x)
    })

    listOfNormalizedDatasets$Training[[i]] = as.data.frame(trainCentered)
    listOfNormalizedDatasets$Testing[[i]] = as.data.frame(testCentered)
    #We need to make sure that there is no comma in any of the file names or this can wreck functions using csvs.
    #We will search for these and remove them. Ideally we could do this once for the entire dataset but this would require
    #checking all variables and then all the levels of all the factor variables. Since this is linear in the number of 
    #features this check shouldn't be computationally difficult so we will do the "lazy" way and check for commas here 
    #and remove them for every fold.
    names(listOfNormalizedDatasets$Training[[i]]) = make.names(names(listOfNormalizedDatasets$Training[[i]]),unique=T)
    names(listOfNormalizedDatasets$Testing[[i]]) = make.names(names(listOfNormalizedDatasets$Testing[[i]]),unique=T)
    }

  return(listOfNormalizedDatasets)
}
```


Following variable imputation, all the variables are normalized (except **time** and **delta**) by taking the scales (mean and standard deviation) of the training set variables and normalizing both the training set and test set with those scales. Here normalizing follows the standard convention of subtracting the mean and dividing by the standard deviation. This function also does some cleaning of variable names -- if a variable name has a comma in it functions later of will break so all commas and other special characters are removed.

Below we normalize the data and show the difference between the data before and after it was normalized.
```{r}
head(imputedData$Training[[1]])
normalizedData = normalizeVariables(imputedData)
head(normalizedData$Training[[1]])
```

###*createFoldsAndNormalize(survivalDataset, numberOfFolds)*

```{r, include=FALSE}
createFoldsAndNormalize = function(survivalDataset, numberOfFolds){
  folds = createFoldsOfData(survivalDataset, numberOfFolds)
  originalIndexing = folds[[1]]
  listOfDatasets = folds[[2]]
  listOfImputedDatasets = meanImputation(listOfDatasets)
  listOfNormalizedDatasets = normalizeVariables(listOfImputedDatasets)
  return(list(originalIndexing, listOfNormalizedDatasets))
}
```

Similar to *validateAndClean()* above, *createFoldsAndNormalize()* serves as a master function which will run *createFoldsOfData()*, *meanImputation()*, and *normalizeVariables()* and will return a list of (2) items, (1) the indexs of the test set as given in Section 2.2.1, and (2) a list containing: (1) a list of normalized training folds and (2) a list of normalized testing folds. 

Here we show the equivalence between *testSetIndexs* from above and the first item of *createFoldsAndNormalize()*  and  equivalence of *normalizedData* with the second item of *createFoldsAndNormalize()*:

```{r}
output = createFoldsAndNormalize(cleanedNACD,5)
all.equal(testSetIndexs, output[[1]])
all.equal(normalizedData, output[[2]])
```

#Creating Individual Survival Distribution  Models

The intention of this work is to examine different survival analysis models which can produce individualized survival distirbutions for different patients. In total we explored five models which produced individual survival distributions and one model which gave a population based survival curve (Kaplan-Meier). Below we give a minimal description for each model and describe how to implement each in the current codebase.


##Kaplan-Meier (KaplanMeier.R)

The Kaplan-Meier (KM) estimator was devised as a nonparametric approach to estimating a survival curve even in the presence of censored data (see the original paper [here](https://web.stanford.edu/~lutian/coursepdf/KMpaper.pdf)). To give the KM estimator we first make the following definitions:

* \(\tau_j\) -- the \(j^\textrm{th}\) distinct death time,
* \(d_j\) -- the number of deaths at time \(\tau_j\),
* \(r_j\) -- the number of patients who were at risk at time \(\tau_j\). Note this also includes those who died at \(\tau_j\).

With these definitions, we say the probability of survival at time \(t\) is given by the KM estimator as
\[\hat{S}_{KM}(t) = \prod_{j: \tau_j < t} \left(1 - \frac{d_j}{r_j}\right)\].
Note that here KM does not take into account any features and thus is considered to a nonparametric model. To give an example of the Kaplan-Meier approach consider the following data:

```{r, echo = FALSE}
library(knitr)
library(kableExtra)
time = c(1,3,5,7,9)
delta = c(1,0,1,1,1)
dat = cbind.data.frame(time,delta)
row.names(dat) = c("Patient 1","Patient 2", "Patient 3","Patient 4","Patient 5")
kable(t(dat), caption = "Example data for the Kaplan-Meier model. Note the data is transposed from the normal format.", format = "html", align = c('c','c','c')) %>%
    kable_styling(bootstrap_options = "striped", full_width = F )
```

Given the data above, we have \(\hat{S}_{KM}(1) = \left(1 - \frac{1}{5}\right) = 0.8\). Note that no change occurs at \(t= 3\) since this patient was censored and \(\tau_j\) only refers to unique *death* times. Below we give the full KM curve for this example.

<center>
```{r, echo = FALSE,message=FALSE, fig.height=3}
library(survival)
library(ggplot2)
library(survminer)
fit = survfit(Surv(time,delta)~1, data = dat)
p = ggsurvplot(fit, data = dat, conf.int = F, legend = "none", palette = "black", censor.size = 10)
p$plot 
```
<figcaption>Figure 2: Kaplan-Meier plot using example data. Censored patients indicated by vertical dash on survival curve.</figcaption>
</center>

Since the KM model does not account for individual features, the survival distribution generated is supposed to represent the population's overall survival curve, *i.e.* the average survival curve. So given \(K\)-fold cross-validation, there will only be \(K\) different survival curves as opposed to the \(N\) different survival curves the other models generate, given \(N\) unique patients. Next we give the details for running KM on a dataset after it has preproccessed by *validateAndClean()* and *createFoldsAndNormalize()*.

###Kaplan-Meier Code (KaplanMeier.R)

```{r, echo =FALSE}
library(prodlim)

KM = function(training, testing){
  kmMod = prodlim(Surv(time,delta)~1, data = training)
  trainingTimes = c(sort(unique(training$time)))
  if(0 %in% trainingTimes){
    times = trainingTimes
  } else{
    times = c(0,trainingTimes)
  }
  probabilities = predict(kmMod,times)
  curvesToReturn = cbind.data.frame(time = times,matrix(rep(probabilities,nrow(testing)),ncol = nrow(testing)))
  timesAndCensTest = cbind.data.frame(time = testing$time, delta = testing$delta)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))   
}
```

**File Purpose**: Use the Kaplan-Meier model that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/KaplanMeier.R".

**Functions**:

1. *KM(training, testing)*

**Arguments**:

* *training*: The training set.
* *testing*: The testing set.

**Packages Required**:

1. survival -- We require the *Surv()* function to change the data into a survival format.
2. prodlim -- prodlim contains a Kaplan-Meier implementation that includes a *predict()* function.

**File Notes**:

Given the list of datasets generated by *createFoldsAndNormalize()*, *KM()* will take a single training and testing set and build the Kaplan-Meier curve for the training set using the *prodlim()* function from the *prodlim* package. The resulting Kaplan-Meier curve is the "predicted" survival distribution for all the testing patients. In addition to the survival curves, *KM()* will also return **time** and **delta** of the testing and training sets. The reason for these additional returned items is for evaluations given in Section ???. 

**Example Usagw**

Here, we give an example of using KM on a single training/testing fold -- extending the results to all folds can be accomplished by using a for loop. 


```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
#Recall the first argument of folds is the indexs of the test set on validatedAndCleaned and the second argument is the list of training and testing sets.
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
kmMod = KM(training1stFold, testing1stFold)
curves = kmMod[[1]]
testDat = kmMod[[2]]
trainDat = kmMod[[3]]
curves
```

Above you will see that the survival curves for all patients are the exact same -- the desired effect for the Kaplan-Meier model. Note that **time** will correspond to all the event times (both death and censored event times) of the *training* data. Additionally we include a **time** = 0 point where the survival probability will be 1 -- this is consistent across all models. 

For a more picturesque approach, we give the survival curve below:

<center>
```{r, echo = FALSE}
library(ggplot2)
library(reshape2)
plotSurvivalCurves = function(survivalCurves, indexToPlot = 1, color = c(), xlim = c()){
  colorOK = T
  if(length(color) == 0)
    colorOK = F
  else if(length(color) != length(indexToPlot)){
    warning("If you would like to select custom colors please make sure the number of colors
            matches the number of curves.")
    colorOK =F 
  }
  time = survivalCurves$time
  curves = survivalCurves[,indexToPlot +1,drop=F]
  plotTimes = seq(min(time),max(time), length.out = length(time)*20)
  plotProbs = as.data.frame(sapply(curves,
                                   function(curve){
                                     curve = ifelse(curve < 1e-20,0,curve)
                                     survivialSpline = splinefun(time, curve, method = "hyman")
                                     return(pmax(survivialSpline(plotTimes),0))
                                   }
  ))
  data = cbind.data.frame(plotTimes,plotProbs)
  longFormData = melt(data,measure.vars = names(data)[-1], variable.name = "Patient")
  plot = ggplot(data = longFormData, aes(x = plotTimes,y = value, colour = Patient))+
    geom_line(size = 1.5)
  if(colorOK)
    plot = plot + scale_color_manual(values = color) 
  if(length(xlim)==2){
    plot = plot+ xlim(c(xlim[1],xlim[2]))
  }
  plot = plot +scale_y_continuous( limits= c(0,1),breaks = c(0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1))+
    theme_bw() +
    theme(text = element_text(size=18, face=  "bold"),
          axis.title = element_text(size = 20),
          axis.title.x = element_text(margin = margin(t = 15)),
          axis.title.y = element_text(margin = margin(r = 15))) + 
    labs(y = "Survival Probaility",x = "Time" )

  return(plot)
}
plotSurvivalCurves(curves, 1:10)
```
</center>

One should notice that the survival curve does not drop to zero and instead ends at \(\approx 0.05\) -- this is because the last patient in the training data was censored. Additionally, the above plot does not follow the classical Kaplan-Meier step function and is smoother using a spline fit -- the details can be found in Section ??? which describes the plotting function.

So far we have shown the output of *curves* but ignored *trainDat* and *testDat* which are just the training and testing values for **time** and **delta**. Across all the following models, *trainDat* and *testDat* will be the same thing, but for clarity we will include them below and not in following models.

<center>**trainDat**</center>
```{r,echo=FALSE}
kable(trainDat, format = "html", align = c('c','c','c')) %>%
    kable_styling(bootstrap_options = "striped", position = "center") %>% 
      scroll_box(width = "65%", height = "200px",extra_css = "margin: auto;") 
```

<br><br>

<center>**testDat**</center>

```{r, echo=FALSE}
kable(testDat, format = "html", align = c('c','c','c')) %>%
    kable_styling(bootstrap_options = "striped", position = "center") %>% 
      scroll_box(width = "65%", height = "200px",extra_css = "margin: auto;") 
```

##Accelerated Failure Time 

The Accelerated Failure Time (AFT) has been a commonly used survival analysis technique for finding significant features in a survival seting. The model is motivated by assuming survival time, \(T\) follows some distribution with density function \(f(t)\) and survival function \(S(t)\). Common AFT distributions are log-logistic, Weibull, log-normal, exponential and more. While we make assumptions on the distribution of \(T\), inference is instead made on \(\log(T)\), that is, for a patient, \(\mathbf{x_i}\) where \(i = 1,2,\ldots N\), AFT models follow the equation:
\[
log(T_i) = \boldsymbol{w}^T\boldsymbol{x_i} + \sigma\epsilon_i,
\]
where \(\boldsymbol{w}\) are feature weights, \(\sigma\) is called a scale parameter and \(\epsilon_i\) is an independent, identically distributed noise parameter dependent on the assumed distribution of \(T\). This is essentially a normal linear regression but on the log scale of \(T\), *i.e.* features are assumed to have a linear relationship with the log of survival time. Here, \(\sigma\) is assumed to be constant across all patients and can be learned by maximum likelihood along with the weights \(\boldsymbol{w}\). 

###Accelerated Failure Time Code (AcceleratedFailureTime.R)
```{r, include=FALSE}
library(survival)

AFT = function(training, testing, AFTDistribution){
  tryCatch({
    AFTMod = survreg(Surv(time,delta)~., data = training, dist = AFTDistribution)
    
    trainingTimes = sort(unique(training$time))
    if(0 %in% trainingTimes){
      timesToPredict = trainingTimes
    } else {
      timesToPredict = c(0,trainingTimes)
    }
    survivalCurves = survfunc(AFTMod, newdata = testing, t = timesToPredict)
  },
  error = function(e) {
    message(e)
    warning("AFT failed to converge.")
  })
  if(!exists("AFTMod") | !exists("survivalCurves")){
    return(NA)
  }

  probabilities = survivalCurves$sur
  #Since survfunc returns survival probabilities with the first time point for every individual (ordered by how the testing individuals)
  #were passed in, we can simply fill a matrix by row to have each individual curve be a column. This can be verified by checking
  #the survival probabilties (sur) for any ID_SurvivalCurves against any column, e.g. the survival probabilities for ID_SurvivalCurves == 2,
  #correspond to the probabilites found in the second column of the matrix below.
  probabilityMatrix = matrix(probabilities, ncol = nrow(testing),byrow = T)
  curvesToReturn = cbind.data.frame(time = timesToPredict, probabilityMatrix)
  timesAndCensTest = cbind.data.frame(time = testing$time, delta = testing$delta)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))   
}

#The following was taken and altered from http://rstudio-pubs-static.s3.amazonaws.com/161203_6ee743eb28df4cd68089a519aa705123.html.
#This code is used to pass in a time point and get the predicted probability. The predict function for survreg objects only return 
#times from probabilities so we need to reverse enginner this using the code below.
survfunc = function (object, t, newdata, name = "t") {
  #Altered from origina: I am going to add an ID to every row so we can retrieve the individuals easily from the output.
  #I gave a weird ID variable name so if the original data came in with a variable ("ID") it won't break our system.
  newdata$ID_SurvivalCurves = 1:nrow(newdata)
  newdata <- do.call(rbind, rep(list(newdata), length(t)))
  t <- rep(t, each = nrow(newdata)/length(t))
  if (class(object) != "survreg") 
    stop("not a survreg object")
  lp <- predict(object, newdata = newdata, type = "lp")
  if (object$dist %in% c("weibull", "exponential")) {
    newdata$pdf <- dweibull(t, 1/object$scale, exp(lp))
    newdata$cdf <- ifelse(t == 0,0,
                          ifelse(is.nan(pweibull(t, 1/object$scale, exp(lp))),1,pweibull(t, 1/object$scale, exp(lp))))
    newdata$haz <- exp(dweibull(t, 1/object$scale, exp(lp), 
                                log = TRUE) - pweibull(t, 1/object$scale, exp(lp), 
                                                       lower.tail = FALSE, log.p = TRUE))
  }
  else if (object$dist == "lognormal") {
    newdata$pdf <- dlnorm(t, lp, object$scale)
    newdata$cdf <- plnorm(t, lp, object$scale)
    newdata$haz <- exp(dlnorm(t, lp, object$scale, log = TRUE) - 
                         plnorm(t, lp, object$scale, lower.tail = FALSE, log.p = TRUE))
  }
  else if (object$dist == "gaussian") {
    newdata$pdf <- dnorm(t, lp, object$scale)
    newdata$cdf <- pnorm(t, lp, object$scale)
    newdata$haz <- exp(dnorm(t, lp, object$scale, log = TRUE) - 
                         pnorm(t, lp, object$scale, lower.tail = FALSE, log.p = TRUE))
  }
  else if (object$dist == "loglogistic") {
    newdata$pdf <- dlogis(log(t), lp, object$scale)/t
    newdata$cdf <- plogis(log(t), lp, object$scale)
    newdata$haz <- exp(dlogis(log(t), lp, object$scale, log = TRUE) - 
                         log(t) - plogis(log(t), lp, object$scale, lower.tail = FALSE, 
                                         log.p = TRUE))
  }
  else if (object$dist == "logistic") {
    newdata$pdf <- dlogis(t, lp, object$scale)
    newdata$cdf <- plogis(t, lp, object$scale)
    newdata$haz <- exp(dlogis(t, lp, object$scale, log = TRUE) - 
                         dlogis(t, lp, object$scale, lower.tail = FALSE, log.p = TRUE))
  }
  else {
    stop("unknown distribution")
  }
  newdata$sur <- 1 - newdata$cdf
  newdata[name] <- t
  return(newdata)
}
```
**File Purpose**: Apply the Accelerated Failure Time model (for a number of distributions) to data that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/AcceleratedFailureTime.R".

**Functions**:

1. *AFT(training, testing, AFTDistribution)*
2. *survfunc(object,t,newdata, name)* - Used to extract survival cures from an AFT model. Altered from [a function previously written by Timothy Johnson.](http://rstudio-pubs-static.s3.amazonaws.com/161203_6ee743eb28df4cd68089a519aa705123.html)


**Arguments**:

* *training*: The training set.
* *testing*: The testing set.
* *AFTDistribution*: The distribution for the AFT model. Possible distributions: *exponential, weibull, lognormal, gaussian, loglogistic, logistic*.
* *object*: A *survreg* object from the *survreg()* function.
* *t*: The times for which to evaluate the AFT model.
* *newdata*: The testing data to predict with using the AFT model.
* *name*: The name of the variable containing the predicted times (*t*).

**Packages Required**:

1. survival -- We use the *survreg()* function to perform analysis for AT models.

**File Notes**:

The primary function for this file is *AFT()*; *survfun()* is a helper function AFT uses to build survival curves for test data after the model has been learned. 

Given the list of datasets generated by *createFoldsAndNormalize()*, *AFT()* will take a single training set, testing set, and a specified distribution and learn the AFT model using the training set. Only one distribution can be tested at a time. In addition, the code contains a tryCatch command the catch the event where the AFT model fails to run, however, this only showed to occur for high dimensional datasets (when no feature selection was applied). 

As with the Kaplan-Meier model, the ouput is a list of three items, (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. 

**Example Usage:**

```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
AFTMod = AFT(training1stFold, testing1stFold, "weibull")
curves = AFTMod[[1]]
curves
```

<br>

<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```
</center>


##Cox-Proportional Hazards with Kalbfleisch-Prentice Extension

The Cox proportional-hazards (Cox-PH) model is extremely common in the survival analysis literature. Previously, we mentioned the Kaplan-Meier model which modeled the survival function, \(S_{KM}(t)\) of a group of patients. Next, the Accelerated Failure Time model assumed a distribution on the survival time, \(T\), which allowed us to include features and build curves for individual patients. Instead, Cox-PH models the *hazard function* of patients, where hazard is interpretted as the risk of failure (dying) at any given time, specifically for a time \(t\) and covariates \(\boldsymbol{x}\) the hazard function is defined as
\[
h(t\,|\,\boldsymbol{x}) = \lim_{\Delta t \rightarrow \,0} \frac{P(t \leq T < t + \Delta t\,\,\, |\,\,\, T \geq t, \boldsymbol{x} )}{\Delta t}. 
\]

Cox-PH models this hazard function in terms of a baseline hazard, \(h_0(t)\), equal for all patients and scaled by a learned, individual risk dependening on features, that is, \(h(t|\boldsymbol{x}) = h_0(t)\,e^{\boldsymbol{w}^T\boldsymbol{x}}\). This is why the model is termed the *proportional hazards* model, the hazard for one patient is proportional to the hazard of all other patients, by a scale depending on individual features.

To learn the values for \(\boldsymbol{w}^T\) one does not need to know the baseline hazard function. Suppose a single patient died at time \(t_j\). The probability that patient \(i\) was the one who died is given by
\begin{align*}
\frac{h(t_j\,|\,\boldsymbol{x_i})}{\sum_{k \in R(t_j)} h(t_j\,|\,\boldsymbol{x_k})} \quad  &= \quad \frac{h_0(t_j)\,e^{\boldsymbol{w}^T\,\boldsymbol{x_i}}}{\sum_{k \in R(t_j)} h_0(t_j)\,e^{\boldsymbol{w}^T\,\boldsymbol{x_k}}}, \\
&= \quad \frac{e^{\boldsymbol{w}^T\,\boldsymbol{x_i}}}{\sum_{k \in R(t_j)} e^{\boldsymbol{w}^T\,\boldsymbol{x_k}}},
\end{align*}
where \(R(t_j)\) is the set of patients still alive (at risk) at time \(t_j\). Thus the likelihood for \(\boldsymbol{w}^T\) is defined as
\[
L\left(\boldsymbol{w}^T\right) = \prod_j \frac{e^{\boldsymbol{w}^T\,\boldsymbol{x_{i\,(j\,)}}}}{\sum_{k \in R(t_j)} e^{\boldsymbol{w}^T\,\boldsymbol{x_k}}},
\]
where subscript \(i (j)\) is interpreted as it is patient \(i\) who died at time \(j\).

By this formulation of the likelihood equation, we see that feature weights are independent of time, *e.g* in a study of survival post surgery, lab test results immedietly following surgery have the same weight one day after surgery as they do a year from surgery. 

While we are able to estimate feature weights without specifying the baseline hazard function, we cannot determine the survival distribution. A number of methods exist for calculating the the baseline hazard and therefore the survival distribution including the Kalbfleisch-Prentice estimator which we use in this work. The thesis ["Baseline Survival Function Estimator under Proporitional Hazards Assumption"](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjploaonvTcAhWKJ3wKHblTAXoQFjAAegQIBBAC&url=http%3A%2F%2Fstat.nuk.edu.tw%2Fgraduate%2F96_paper%2FM0944411.pdf&usg=AOvVaw1LAbWJ2e1MQL2KIXBPqIZA) by Weng provides a nice disucssion of the Breslow and Kalbfleisch-Prentice estimators.


###Cox-Proportional Hazards with Kalbfleisch-Prentice Extension Code (CoxPH_KP.R)
```{r, echo=FALSE}
#Library Dependencies
#survival is needed to get survfit and the implimentation of the KP estimator.
library(survival)
#For EN-Cox
library(fastcox)
#For sindex
library(prodlim)

CoxPH_KP = function(training, testing,ElasticNet=F){
  if(ElasticNet){
    timeInd = which(names(training) == "time")
    deltaInd = which(names(training) == "delta")
    alpha = NULL
    lambda = NULL
    bestError = Inf
    for(i in c(0.01,.2,.4,.6,.8,1)){
      model =  cv.cocktail(as.matrix(training[,-c(timeInd, deltaInd)]),training[,timeInd], training[,deltaInd],alpha = i)
      modelBestLambdaIndex = which(model$lambda == model$lambda.min)
      modelError = model$cvm[modelBestLambdaIndex]
      if(modelError < bestError){
        alpha = i
        lambda = model$lambda.min
        bestError = modelError
      }
    }
    coxModel = cocktail(as.matrix(training[,-c(timeInd, deltaInd)]),training[,timeInd], training[,deltaInd],alpha = alpha,lambda = lambda)
    linearPredictionsTraining = predict(coxModel,as.matrix(training[,-c(timeInd, deltaInd)]),type = "link")
    linearPredictionsTesting = predict(coxModel,as.matrix(testing[,-c(timeInd, deltaInd)]),type = "link")
    survivalEstimate = KPEstimator(linearPredictionsTraining, training$time,training$delta)
    survCurvs = t(sapply(survivalEstimate[[2]], function(x) x^exp(linearPredictionsTesting)))
    survivalCurves = list(time = survivalEstimate[[1]], surv = survCurvs)
  }
  else{
    tryCatch({
      coxModel = coxph(Surv(time,delta)~., data = training,singular.ok = T)
      survivalCurves = survfit(coxModel, testing, type = "kalbfleisch-prentice")
    },
    error = function(e) {
      message(e)
      warning("Cox-PH failed to converge (likely due to singularity). Future runs have been eliminated for Cox.")
    })
    if(!exists("coxModel") | !exists("survivalCurves")){
      return(NA)
    }
  }
  if(0 %in% survivalCurves$time){
    timePoints = survivalCurves$time
    probabilities = survivalCurves$surv
  } else{
    timePoints = c(0,survivalCurves$time)
    probabilities = rbind(1,survivalCurves$surv)
  }
  curvesToReturn = cbind.data.frame(time = timePoints, probabilities)
  timesAndCensTest = cbind.data.frame(time = testing$time, delta = testing$delta)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))  
}

#not considering ties.
KPEstimator = function(lp,lpTime,censorStatus){
  indexToKeep = sindex(sort(lpTime), unique(sort(lpTime)))
  orderLPTime = order(lpTime)
  cumHaz = rev(cumsum(rev(exp(lp[orderLPTime]))))
  alpha = ((1-(exp(lp[orderLPTime])/cumHaz)))^exp(-lp[orderLPTime])
  survivalFunc = cumprod(alpha^censorStatus[orderLPTime])
  return(list(time = lpTime[orderLPTime][indexToKeep],surv = survivalFunc[indexToKeep]))
}
```

**File Purpose**: Apply the Cox Proportional-Hazards model to data that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/CoxPH_KP.R".

**Functions**:

1. *CoxPH_KP(training, testing, ElasticNet)*
2. *KPEstimator(lp, lpTime, censorStatus)* - Used to estimate the Kalbfleisch-Prentice estimator when doing elastic-net Cox.

**Arguments**:

* *training*: The training set.
* *testing*: The testing set.
* *ElasticNet*: A boolean specifying whether or not to
* *lp*: The linear predictions (\(\boldsymbol{w}^T\boldsymbol{x}\)) from the Cox-PH model for the training data.
* *lpTime*: The training dataset event times, *i.e.* **time**.
* *censorStatus*: The training dataset censor status, *i.e* **delta**.

**Packages Required**:

1. survival -- Needed for *coxph()* and *survfit()*.
2. fastcox -- See Elastic-Net Cox.
3. prodlim -- See Elastic-Net Cox.

**File Notes**:

The primary function for this file is *CoxPH_KP()*; *KPEstimator()* is only used for the elastic-net Cox model.

Given the list of datasets generated by *createFoldsAndNormalize()*, *CoxPH_KP()* will take a single training set, testing set, and a boolean specifying whether or not to use the elastic-net model. For this section we assume ElasticNet = FALSE, that is we are NOT using the elastic net formulation. The code uses a tryCatch function to avoid models which are failing to converge (this happened for high dimensional datasets). The Cox-PH model is fit using *coxph()* from the survival package and then survival curves are fit using *survfit()* and specifying the type to be the Kalbfleisch-Prentice estimator. Ties are accounted for using the default of *coxph()*, *i.e.* the Efron approximation.

As with preious models, the ouput is a list of three items, (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. 

**Example Usage:**

```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
CoxMod = CoxPH_KP(training1stFold, testing1stFold, ElasticNet=F)
curves = CoxMod[[1]]
curves
```
<br>

<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```
</center>

Note the preceding Figure depicts proportional hazards -- all curves are the exact same shape but at different heights, *i.e.* they are proportional to one another. 

##Cox-Proportional Hazards with Kalbfleisch-Prentice Extension -- Elastic Net

We include the elastic net version of Cox-PH to include a regularized version of Cox-PH to compete with other models such as Random Survival Forests and Multi-Task Logistic Regression which have built in regularization. The notation and implementations of elastic net come from Yang and Zou's paper entitiled ["A Cocktail Algorithm for Solving The Elastic Net Penalized Cox's Regression in High Dimensions""](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwiG3rLYuvTcAhUBN30KHfIQDYUQFjAAegQIABAC&url=http%3A%2F%2Fusers.stat.umn.edu%2F~zouxx019%2FPapers%2Ffastcox.pdf&usg=AOvVaw21oG6xeyfTb7jaKFBkYJct). The elastic net penalty adds the following term to the likelihood equation,
\[
P_{\lambda, \alpha}(\boldsymbol{w}) = \sum_{j=1}^p \lambda\left(\alpha\,|w_j| + \frac{1}{2}\,(1-\alpha)\, w_j^2 \right),
\]
for \(p\) features where \(\lambda > 0\) and \(0 < \alpha \leq 1\). The first penalty term, \(|w_j|\) corresponds to the LASSO loss and is responsible for feature selection whereas the quadratic term is to control the size of feature weights. For example, \(\alpha = 1\) would correspond to just LASSO-Cox, whereas \(\alpha \approx 0\) would correspond to Ridge-Cox. 

For more information regarding elastic net Cox see the Yang and Zou's paper referenced above.

###Cox-Proportional Hazards with Kalbfleisch-Prentice Extension -- Elastic Net Code (CoxPH_KP.R)

**File Purpose**: Apply the Cox Proportional-Hazards model to data that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/CoxPH_KP.R".

**Functions**:

1. *CoxPH_KP(training, testing, ElasticNet)*
2. *KPEstimator(lp, lpTime, censorStatus)* - Used to estimate the Kalbfleisch-Prentice estimator when doing elastic net Cox.

**Arguments**:

* *training*: The training set.
* *testing*: The testing set.
* *ElasticNet*: A boolean specifying whether or not to
* *lp*: The linear predictions (\(\boldsymbol{w}^T\boldsymbol{x}\)) from the Cox-PH model for the training data.
* *lpTime*: The training dataset event times, *i.e.* **time**.
* *censorStatus*: The training dataset censor status, *i.e* **delta**.

**Packages Required**:

1. survival -- Needed for the normal Cox implementation.
2. fastcox -- Used for the implementation of elastic net cox (*cocktail()*)
3. prodlim -- Used for the *sindex()* function -- sindex returns the indexs of positions, ideal for evaluating step functions.

**File Notes**:

Given the list of datasets generated by *createFoldsAndNormalize()*, *CoxPH_KP()* will take a single training set, testing set, and a boolean specifying whether or not to use the elastic-net model. For this section we assume ElasticNet = TRUE, that is we are using the elastic net formulation. There are two hyperparameters involved for elastic net Cox, the strength of the penalty, \(\lambda\), and the type of penalty (more LASSO like or more Ridge like), \(\alpha\). Six values are tested for \(\alpha\), 0.01, 0.2, 0.4, 0.6, 0.8, and 1.0. For each of these values the function *cv.cocktail()* tests 100 values of \(\lambda\) using 5-fold cross validation and returns \(\lambda_{min}\), the optimal value of \(\lambda\) to minimize the cross validation error (see the *cocktail()* function of the [package documentation](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=2ahUKEwjCmMChxvTcAhUMIjQIHTumDoEQFjABegQICRAC&url=https%3A%2F%2Fcran.r-project.org%2Fweb%2Fpackages%2Ffastcox%2Ffastcox.pdf&usg=AOvVaw18r4av9gbxAKwh-LuEP3j5)).The optimal value for \(\alpha\) and \(\lambda\) from interval 5-fold cross validation are then used for evaluting the test fold. Note the optimal values for \(\alpha\) and \(\lambda\) will (likely) differ across folds.

One the feature weights have been estimated, linear predictors are passed to *KPEstimator()* to estimate the survival curves. As of now, ties are not accounted for and the resulting survival distribution are using the estimate derived in the pressence of no ties. THIS NEEDS TO BE UPDATED TO EITHER USING THE BRESLOW ESTIMATOR OR NUMERICALLY GETTING ESTIMATING SURVIVAL CURVES IN THE PRESENCE OF TIES.

As with preious models, the ouput is a list of three items, (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. 

**Example Usage:**

```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
CoxMod = CoxPH_KP(training1stFold, testing1stFold, ElasticNet=T)
curves = CoxMod[[1]]
curves
```
<br>

<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```
</center>



##Random Survival Forest - Kaplan-Meier Extension 

[Random Survival Forests](https://arxiv.org/pdf/0811.1645) (RSFs) were developed by Ishwaran et al. in 2008 and have been growing in popularity in recent years. While we do not give vast detail regarding the formation of these forests, the [original paper](https://arxiv.org/pdf/0811.1645) and [R package details](https://kogalur.github.io/randomForestSRC/theory.html) provide an excellent overview. 

The primary focus for RSFs were to build a risk score using an averaged cumulative hazard from the terminal nodes which in which patients landed. As previously mentioned here and explained further in our paper, risk scores are useful for discriminating between patients, but do not provide individual survival distributions.While survival distributions are not mentioned in the original RSF paper, the R package details briefly describe how RSFs build individual survival distributions in Section 8.1. Specifically, they state survival estimates are computed using Kaplan-Meier estimators. However, given a total of \(M\) trees in a forest, there will be \(M\) Kaplan-Meier estimators for each patient and the details of how these Kaplan-Meier estimators were combined are not given. After some backwards engineering I was able to come up with the conclusion that Kaplan-Meier curves are "averaged" across all the Kaplan-Meier curves - see Figure below.


<center>
```{r, echo=FALSE}
time1 = c(3,5,8,10,15)
cens1 = c(1,1,1,1,1)
time2 = c(6,9,10,17,20)
cens2 = c(1,1,1,1,1)
dat = cbind.data.frame(time1,cens1,time2,cens2)


p1 = prodlim(Surv(time1,cens1)~1,data = dat)
p2 = prodlim(Surv(time2,cens2)~1, data = dat)
KM1 = predict(p1, seq(0,20,by = 0.01)) 
KM2 = predict(p2, seq(0,20,by = 0.01))
KM1 = ifelse(is.na(KM1),0,KM1)
KM2 = ifelse(is.na(KM2),0,KM2)
RKM = (KM1 + KM2)/2

dat2 = cbind.data.frame(time = seq(0,20,by=0.01), RKM = RKM, KM1 = KM1, KM2 = KM2)

ggplot(data = dat2)+
  geom_line(aes(x = time, y = KM1), color = "dodgerblue2",size = .8)+
  geom_line(aes(x = time, y = KM2), color = "dodgerblue2", size = .8)+
  geom_line(aes(x = time, y = RKM), color = "orangered2",size = 1) +
  theme_bw()+
  labs(x = "Time",y = "Survival Probability")+
  theme(text = element_text(size = 18, face = "bold"))

```
</center>

<figcaption>Figure: Depiction of averaing two Kaplan-Meier curves together to produce one individual survival distribution for a test patient. Blue Kaplan-Meier curves represent the Kaplan-Meier estimators in two terminal nodes whereas the red curve represents the combination - giving form to the individual survial distribution.</figcaption>

###Random Survival Forest - Kaplan-Meier Extension Code (RandomSurvivalForests.R)
```{r, include = FALSE}
library(survival)
library(randomForestSRC)

RSF = function(training, testing, ntree){
  rsfMod = rfsrc(Surv(time,delta)~., data = training, ntree = ntree)
  survivalCurves = predict(rsfMod, testing)
  trainingTimes = survivalCurves$time.interest
  if(0 %in% trainingTimes){
    times = trainingTimes
    probabilities = t(survivalCurves$survival)
  } else{
    times = c(0,trainingTimes)
    probabilities = rbind.data.frame(1,t(survivalCurves$survival))
  }
  curvesToReturn = cbind.data.frame(time = times, probabilities)
  timesAndCensTest = cbind.data.frame(time = testing$time, delta = testing$delta)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))  
}
```


**File Purpose**: Apply the RSF model to data that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Path**: "ISDEvaluationPipeline/Models/RandomSurvivalForests.R".

**Functions**:

1. *RSF(training, testing, ntree)*

**Arguments**:

* *training*: The training set.
* *testing*: The testing set.
* *ntree*: The number of trees.

**Packages Required**:

1. survival -- Needed for the *Surv()* function
2. randomForestSRC -- Used for the RSF implementation.

**File Notes**:

Given the list of datasets generated by *createFoldsAndNormalize()*, *RSF()* will take a single training set, testing set, and an integer specifying the number of trees to use in the forest and build the RSF model using *rfsrc()* from the randomForestSRC package. Then the *predict()* functon for rfsrc is used to build survival curves for test patients.

As with previous models, the ouput is a list of three items, (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. 

**Example Usage:**

```{r, warning=FALSE}
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
rsfMod = RSF(training1stFold, testing1stFold, ntree = 1000)
curves = rsfMod[[1]]
names(curves) = c("time",1:26)
curves
```
<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```
</center>


##Multi-task Logistic Regression (MTLR.R)

Of the models tested, Multi-task Logistic Regression (MTLR) is the least well known so we give a brief description here. Consider modeling the probability of survival of patients at each of a vector of time points
$\tau = [t_1, t_2, \ldots, t_m]$, *e.g.* $\tau$ could be the 60 monthly intervals from 1 month up to 60 months. 
We can set up a series of logistic regression models:
For each patient, representated as $\vec{x}$,
\[
P(T \geq t_i \,|\, \vec{x}) =  \left(1 + \exp(\vec{\theta_{i}}\cdot \vec{x} )\right)^{-1}
\]
where $\vec{\theta_i}$  are the time-specific feature vectors.
While the input features $\vec{x}$ stay the same for all these classification tasks,  
the binary labels $y_i = [T\geq t_i]$ can change depending on the threshold $t_i$.%

We encode the survival time $d$ of a patient as a binary sequence 
$y = y(d) = (y_1, y_2, \ldots, y_m)$, where $y_i = y_i(d) \in \{0,1\}$ denotes the survival status of the patient at time $t_i$, 
so that $y_i = 0$ (no death event yet) for all $i$ with $t_i <d$, and $y_i = 1$ (death) for all $i$ with $t_i \geq d$.



Here there are $m+1$ possible legal sequences of the form $(0,0,\ldots,1,1,\ldots,1)$, including the sequence of all 0's and the sequence of all 1's. 
The probability of observing the survival status sequence $y = (y_1, y_2, \ldots, y_m)$ can be represented as:
\[
  P_\Theta(Y\!\! =\!\! (y_1, y_2, \ldots, y_m) \,\,|\,\, \vec{x}) = \frac{\exp(\sum_{i=1}^m y_i \times \theta_{i}\cdot\vec{x} )}
     {\sum_{k=0}^m \exp(f_{\Theta}(\vec{x}, k))}, 
\]
where $\Theta = (\theta_{1}, \ldots, \theta_{m})$, and
$f_{\Theta}(\vec{x}, k) = \sum_{i=k+1}^m (\theta_{i}\cdot\vec{x})$ 
for $0\leq k\leq m$ is the score of the sequence with the event occurring in the interval $[t_k, t_{k+1})$ before taking the logistic transform, with the boundary case $f_{\Theta}(\vec{x}, k)= 0$ being the score for the sequence of all `0's. 
Given a dataset of $n$ patients $\{ \vec{x_r}\}$ with associated time of deaths $\{ d_r \}$, 
we find the optimal parameters (for the \MTLR\ model) $\Theta^*$ as
\[
\Theta^*\ =\
\arg\max_{\Theta} 
\sum_{r=1}^n \left[\sum_{i=1}^m y_j(d_r)(\theta{i}\!\cdot\!\vec{x_r})\! - \log \sum_{k=0}^m \exp f_{\Theta}(\vec{x_r},k) \right]
  -  
\frac{C}{2}\!\sum_{j=1}^m\! \|\theta_{j}\|^2\! 
% +\! \frac{C_2}{2}\sum_{j=1}^{m-1}\! \|\theta_{j+1}\!-\!\theta_j\|^2\!
\]
where the $C$ (for the regularizer) is found by an internal cross-validation process.
}


There are many details here -- \eg 
to insure that the survival function starts at 1.0, and decreases monotonically and smoothly until reaching 0.0 for the final time point; 
to deal appropriately with censored patients; 
to decide how many time points to consider ($m$); and 
to minimize the risk of overfitting (by regularizing),
and by selecting the relevant features.
The [paper by Yu et al.](https://papers.nips.cc/paper/4210-learning-patient-specific-cancer-survival-distributions-as-a-sequence-of-dependent-regressors) provides the details.


###Multi-task Logistic Regression Code (MTLR.R)
```{r, include = FALSE}
MTLR = function(training, testing,addLastTimePoint=F){
  #The idea here is to have the working directory sitting in ISDEvaluationPipeline. We move the working directory into the folder
  #with executables for ease of execution and move back to the original working directory before exiting the function.
  executablesPath = "Models/AdditionalMTLRFiles/"
  originalWd = getwd()
  setwd(paste(originalWd,"/",executablesPath,sep=""))
  #Write csv files to be called by CovertDataFiles to make the correct format of input for MTLR.
  write.csv(training, paste("training.csv",sep=""), row.names = F)
  write.csv(testing, paste("testing.csv",sep=""), row.names = F)
  system2("java", args=c('-cp ./',"ConvertDataFiles", "convert2MTLR", "training.csv", "training.mtlr", "FlipCensoredBit"))
  system2("java", args=c('-cp ./',"ConvertDataFiles", "convert2MTLR", "testing.csv", "testing.mtlr", "FlipCensoredBit"))
  system2("./mtlr_opt",args=c("-i", "training.mtlr"),stdout = FALSE)
  system2("./mtlr_test", args=c("-i", "testing.mtlr", "-s","training.mtlr", "-o", "./fold1_modelfile > MTLR_output.txt"))
  times = unlist((unname(read.table("fold1_modelfile",skip = 1,sep = ",",nrows = 1))))
  if(addLastTimePoint){
    #It appears the last survival probability is always appearing to be zero. We need to add some additional time point to the end to account
    #for this so we choose the value of the last time point  squared divided by the second to last time point. 
    lastTimePoint = round(times[length(times)]^2/times[length(times) -1])
    #Add the last time point and a 0 time point if needed.
    if(0 %in% times){
      timePoints = c(times,lastTimePoint)
    } else{
      timePoints = c(0,times, lastTimePoint)
    } 
  }
  else{
    if(0 %in% times){
      timePoints = times
    } else{
      timePoints = c(0,times)
    } 
  }

  testingPoints = read.table("MTLR_output.txt")
  #Clean up directory:
  system2("rm",args=c("fold1_modelfile","CI_log","Ptrain1", "Pmodel1", "*.csv","*.mtlr","*.txt"))
  #Replace original working directory.
  setwd(originalWd)
  #the first 4 columns are the true time of death, 1- censoring status, and 2 different averaged survival times.
  trueDeathTimes = testingPoints[,1]
  censorStatus = 1 -testingPoints[,2]
  #We don't want the averages (first 3 columns) and the last column is some evaluation of survival probability at the true time of death.
  #We will discard this since later on we have a method used for every survival curve. Further we minus 1 because we dont want to include the 
  #0th time point, i.e. since we use length(timePoints) we need to subtract an extra value it since we included a 0.
  #So we have ignore first 4 columns through the number of time points, subtracting 1 for the 0th time point (if we included a zero).
  if(0 %in% times){
      survivalProbabilities = testingPoints[5:(4+length(timePoints))]
  } else{
      survivalProbabilities = testingPoints[5:(4+length(timePoints)-1)]
    }
  #Survival probabilities were read in a factors and contain commas. We will clean this out by turning them into character vectors and 
  #trimming commas.
  survivalProbabilities = apply(survivalProbabilities,c(1,2),as.character)
  survivalProbabilities = apply(survivalProbabilities,c(1,2), function(x) as.numeric(gsub(",","",x)))
  #Some of the last survival probabilities were negative so we turned these to zero. There were values like -1.2239e-17 so effectively 
  #zero anyways. Additionally we need to transpose the survival probabilities to match up with the survival time estimates and then
  #add a survival probability of 1 to the 0th time point if there was no original 0 time point.
  if(0 %in% times){
    survivalProbabilities = t(apply(survivalProbabilities, c(1,2), function(x) ifelse(x < 0,0,x)))
  } else {
    survivalProbabilities = rbind(1,t(apply(survivalProbabilities, c(1,2), function(x) ifelse(x < 0,0,x))))
  }
  curvesToReturn = cbind.data.frame(time = timePoints, survivalProbabilities) 
  timesAndCensTest = cbind.data.frame(time = trueDeathTimes, delta = censorStatus)
  timesAndCensTrain = cbind.data.frame(time = training$time, delta = training$delta)
  return(list(curvesToReturn, timesAndCensTest,timesAndCensTrain))  
}
  
  
```


**File Purpose**: Apply the MTLR model to data that has been processed and split into folds by *createFoldsAndNormalize()*.

**File Paths**: 

1. "ISDEvaluationPipeline/Models/MTLR.R".
2. "ISDEvaluationPipeline/Models/AdditionalMTLRFiles/ConvertDataFiles.class"
3. "ISDEvaluationPipeline/Models/AdditionalMTLRFiles/ConvertDataFiles.java"
4. "ISDEvaluationPipeline/Models/AdditionalMTLRFiles/mtlr_opt"
5. "ISDEvaluationPipeline/Models/AdditionalMTLRFiles/mtlr_test"
6. "ISDEvaluationPipeline/Models/AdditionalMTLRFiles/mtlr_train"

**Functions**:

1. *MTLR(training, testing, addLastTimePoint=F)*

**Arguments**:

* *training*: The training set.
* *testing*: The testing set.
* *addLastTimePoint*: The output of MTLR gives a final probability point of \(\approx 0\) -- a last time point can be associated with this where the last time point added is equal to the last time point (included in the analysis) squared divided by the second to last time point.

**Packages Required**: None.

**File Notes**:

Given the list of datasets generated by *createFoldsAndNormalize()*, *MTLR()* will take a single training set, testing set, and a boolean specifying whether to add a time point for the zero probability point. There is no R package (yet) associated with MTLR so the R code is a bit of a work around. The current implementation of MTLR is in C++ so here we simply use the executables from that code. Prior to using the executables, data must be in a special format which can be made by using the "ConvertDataFiles.class" file (the .java file is also included for transparency). One data has been put in .mtlr format, MTLR will be trained using the mtlr_opt executable and can be tested using mtlr_test.  Following this the times and probabilities are extracted and a final time point is added for a probability of 0 point, and otherwise the 0 probability point is removed. The executables create some intermediary files which are cleaned out near the end of the MTLR.R command.

For the source code associated with MTLR as well as a web-tool to use MTLR see the [Patient Specific Survival Prediction (PSSP) website.](http://pssp.srv.ualberta.ca)

As with previous models, the ouput is a list of three items, (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. 

**Example Usage:**

```{r, warning=FALSE}
setwd("./ISDEvaluationPipeline")
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
mtlrMod = MTLR(training1stFold, testing1stFold, addLastTimePoint = FALSE)
curves = mtlrMod[[1]]
row.names(curves) = NULL
curves

mtlrModLastTimePoint = MTLR(training1stFold, testing1stFold, addLastTimePoint = TRUE)
curvesLastTimePoint = mtlrModLastTimePoint[[1]]
row.names(curvesLastTimePoint) = NULL
curvesLastTimePoint
```
<center>
```{r, echo=FALSE}
plotSurvivalCurves(curves, 1:10)
```
<figcaption>Figure: MTLR Survival curves **without** last time point added.</figcaption>
</center>
<br><br>
<center>
```{r, echo=FALSE}
plotSurvivalCurves(curvesLastTimePoint, 1:10)
```
<figcaption>Figure: MTLR Survival curves **with** last time point added.</figcaption>
</center>


#Evaluting ISD Models

Once the survival curves have been generated it is likely beneficial to evaluate them using some type of metric. We tested (5) different metrics, Concordance (a discriminatory measure), 1-Calibration, Brier score, L1-Loss and novel D-Calibration. Below we only give the details for the code -- for explanations of each metric please see the paper associated with this tutorial (Link). In addition to the evaluation metrics we also describe the function used to calculate the mean/median survival from each survival curve. We will show all evaluations using the unregularized Cox model as this is one of the most common models used in survival analysis.

##Calculating ``Average'' Survival (EvaluationHelperFunctions.R)

```{r, include = FALSE}
predictProbabilityFromCurve = function(survivalCurve,predictedTimes, timeToPredict){
  spline = splinefun(predictedTimes, survivalCurve, method = "hyman")
  maxTime = max(predictedTimes)
  slope = (1-spline(maxTime))/(0 - max(predictedTimes))
  predictedProbabilities = rep(0, length(timeToPredict))
  linearChange = which(timeToPredict > maxTime)
  if(length(linearChange) > 0){
    predictedProbabilities[linearChange] = pmax(1 + timeToPredict[linearChange]*slope,0)
    predictedProbabilities[-linearChange] = spline(timeToPredict[-linearChange])
  }
  else{
    predictedProbabilities = spline(timeToPredict)
  }
  return(predictedProbabilities)
}

#We calculate the mean and median survival times assuming a monotone spline fit of the survival curve points.
predictMeanSurvivalTimeSpline = function(survivalCurve, predictedTimes){
  #If all the predicted probabilities are 1 the integral will be infinite. For this reason we slightly decrease the 
  #last value.
  if(all(survivalCurve==1)){
    return(Inf)
  }
  spline = splinefun(predictedTimes, survivalCurve, method = "hyman")
  maxTime = max(predictedTimes)
  slope = (1-spline(maxTime))/(0 - max(predictedTimes))
  zeroProbabilitiyTime = min( predictedTimes[which(survivalCurve ==0)], maxTime + (0-spline(maxTime))/slope)
  splineWithLinear = function(time) ifelse(time < maxTime, spline(time),1 + time*slope)
  area = integrate(splineWithLinear,0, zeroProbabilitiyTime,subdivisions = 1000,rel.tol = .001)[[1]]
  return(area)
}

predictMedianSurvivalTimeSpline = function(survivalCurve, predictedTimes){
  #If all the predicted probabilities are 1 the integral will be infinite.
  if(all(survivalCurve==1)){
    return(Inf)
  }
  spline = splinefun(predictedTimes, survivalCurve, method = "hyman")
  minProb = min(spline(predictedTimes))
  if(minProb < .5){
    maximumSmallerThanMedian = predictedTimes[min(which(survivalCurve <.5))]
    minimumGreaterThanMedian = predictedTimes[max(which(survivalCurve >.5))]
    splineInv = splinefun(spline(seq(minimumGreaterThanMedian, maximumSmallerThanMedian, length.out = 1000)),
                          seq(minimumGreaterThanMedian, maximumSmallerThanMedian, length.out = 1000))
    medianProbabilityTime = splineInv(0.5)
  }
  else{
    maxTime = max(predictedTimes)
    slope = (1-spline(maxTime))/(0 - max(predictedTimes))
    medianProbabilityTime = maxTime + (0.5-spline(maxTime))/slope
  }
  return(medianProbabilityTime)
}
```

**File Purpose**: Calculate the mean/median survival time given a single survival curve.

**File Paths**: 

1. "ISDEvaluationPipeline/Evaluations/EvaluationHelperFunctions.R"

**Functions**:

1. *predictProbabilityFromCurve(survivalCurve,predictedTimes, timeToPredict)*
2. *predictMeanSurvivalTimeSpline(survivalCurve, predictedTimes)*
3. *predictMedianSurvivalTimeSpline(survivalCurve, predictedTimes)*

**Arguments**:

1. *survivalCurve*: The survival probabilities generated for a individual survival curve.
2. *predictedTimes*: The times corresponding to survival probabilities, *e.g.* time = 0 corresponds to a survival probability of 1.
3. *timeToPredict*: The time for which to extract a survival probability given a survival curve. Note this can either be a single time or a vector of times.

**Packages Required**: None.

**File Notes**: 

This files purpose is to provide support for the evaluation metrics. Some metrics (concordance, L1-loss) require a single risk/survival measure and for those we use the mean (or median) survival times, *predictMeanSurvivalTimeSpline()* and *predictMedianSurvivalTimeSpline()* accordingly. To predict the mean time a monotonic spline fit is used to make the survival probabilities continuous. Specifically, we use the Hyman filtering of the cubic Hermite spline (see the [Wikipedia page](https://en.wikipedia.org/wiki/Monotone_cubic_interpolation) and the page's references for an introduction). However, if the survival curves do not go to 0 probability we use a linear fit from the time = 0, survival probability = 1 point to the last time point we have and extend this linear fit to zero, see Figure X below. Once the survival curves have been extended to zero the integral of the survival curve is taken to be the mean survival time.

![Figure X: Linear fitting to extend survival curves to 0 survival probability. Left: Survival probabilities with no linear fit, Right: Survival probabilityies with linear fitting.](NoLinearVsLinear.png)

For calculating the median survival time the same spline fit is used and then an inverse spline is fit to the original spline fitting. The time associated with a survival probability of 0.5 is used as the median survival time.

Additionally, some evaluation metrics (specifically calibration metrics) require survival probabilities for a given time. This is implemented by *predictProbabilityFromCurve()*. The same spline fit (with linear extension to zero) from before is fitted to the survival probabilites and the given time(s) to predict (timeToPredict) is calculated using the internal *spline()* function. 

**Example Usage**:
```{r, include = FALSE}
plotSurvivalCurvesWithLinear = function(survivalCurves, indexToPlot = 1, color = c(), xlim = c()){
  colorOK = T
  if(length(color) == 0)
    colorOK = F
  else if(length(color) != length(indexToPlot)){
    warning("If you would like to select custom colors please make sure the number of colors
            matches the number of curves.")
    colorOK =F 
  }
  time = survivalCurves$time
  curves = survivalCurves[,indexToPlot +1,drop=F]
  plotTimes = seq(min(time),max(time), length.out = length(time)*20)
  plotProbs = as.data.frame(sapply(curves,
                                   function(curve){
                                     curve = ifelse(curve < 1e-20,0,curve)
                                     survivialSpline = splinefun(time, curve, method = "hyman")
                                     return(pmax(survivialSpline(plotTimes),0))
                                   }
  ))
  maxProb = max(plotProbs[nrow(plotProbs),])
  maxSlope = (1 - maxProb)/(0 - max(plotTimes))
  maxLinearTime = max(plotTimes) + (0 - maxProb)/maxSlope
  additionalPlotTimes = seq(max(plotTimes), maxLinearTime, length.out = ceiling((maxLinearTime - max(plotTimes)))*20)
  plotLinearProbs = as.data.frame(sapply(plotProbs,
                                   function(probs){
                                     slope = (1 - min(probs))/(0 - max(plotTimes))
                                     return(pmax(min(probs) + (additionalPlotTimes - max(plotTimes))*slope,0))
                                   }
  ))
  plotTimes = c(plotTimes, additionalPlotTimes)
  plotProbs = rbind.data.frame(plotProbs, plotLinearProbs)
  data = cbind.data.frame(plotTimes,plotProbs)
  longFormData = melt(data,measure.vars = names(data)[-1], variable.name = "Patient")
  plot = ggplot(data = longFormData, aes(x = plotTimes,y = value, colour = Patient))+
    geom_line(size = 1.5)
  if(colorOK)
    plot = plot + scale_color_manual(values = color) 
  if(length(xlim)==2){
    plot = plot+ xlim(c(xlim[1],xlim[2]))
  }
  plot = plot +scale_y_continuous( limits= c(0,1),breaks = c(0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1))+
    theme_bw() +
    theme(text = element_text(size=18, face=  "bold"),
          axis.title = element_text(size = 20),
          axis.title.x = element_text(margin = margin(t = 15)),
          axis.title.y = element_text(margin = margin(r = 15))) + 
    labs(y = "Survival Probaility",x = "Time" )
  
  return(plot)
}


```

```{r, warning=FALSE}
#Set up:
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
CoxMod = CoxPH_KP(training1stFold, testing1stFold, ElasticNet=F)
curves = CoxMod[[1]]


survivalTimes = curves[,1]
#We use the second patient because the first had a very shallow survival curve - see Cox curves in Seciton 3.3.1.
secondSurvivalCurve = curves[,3]

#Getting the survival probability for 5 months:
predictProbabilityFromCurve(survivalCurve = secondSurvivalCurve, predictedTimes = survivalTimes, timeToPredict = 5)
#Getting multiple survival probabilities:
predictProbabilityFromCurve(survivalCurve = secondSurvivalCurve, predictedTimes = survivalTimes, timeToPredict = c(5,10,20,40,80,160))

#Getting the mean survival time:
predictMeanSurvivalTimeSpline(survivalCurve = secondSurvivalCurve, predictedTimes = survivalTimes)

#Getting the median survival time:
predictMedianSurvivalTimeSpline(survivalCurve = secondSurvivalCurve, predictedTimes = survivalTimes)
```
<center>
```{r, echo=FALSE,}
plotSurvivalCurvesWithLinear(curves,2)
```
</center>
##Concordance (Concordance.R)

```{r, include= FALSE}
library(survival)

#Helper Functions: predictMeanSurvivalTimeLinear(survivalCurve,predictedTimes)
source("ISDEvaluationPipeline/Evaluations/EvaluationHelperFunctions.R")

#The following function is split into 2 parts. Part 1 retrieves all the relevant pieces from the passed in survMod object, e.g. the survival
#curves and the true death times of test subjects. Part 2 uses survConcordance to calculate classical concordance measures. Additionally, this is were tied data is handled.
Concordance = function(survMod, ties = "None", method = "Mean"){
  #Part 1:
  #Being passed an empty model.
  if(is.null(survMod)) return(NULL)
  #Being passed a model that failed.
  suppressWarnings(if(is.na(survMod[[1]])) return(NULL))
  if(!ties %in% c("None","Risk","Time","All"))
    stop("Please enter one of: 'None', 'Risk','Time', or 'All' as the ties argument.")
  predictedTimes = survMod[[1]][,1]
  survivalCurves = survMod[[1]][-1]
  trueDeathTimes = survMod[[2]][,1]
  censorStatus = survMod[[2]][,2]
  
  predictMethod = switch(method,
                         Mean = predictMeanSurvivalTimeSpline,
                         Median = predictMedianSurvivalTimeSpline)
  #This retrieves the mean death probability of the survival curve.
  averageSurvivalTimes = unlist(lapply(seq_along(trueDeathTimes),
                                       function(index) predictMethod(survivalCurves[,index],
                                                                                     predictedTimes)))
  #Part 2:
  #The risk score should be higher for subjects that live shorter (i.e. lower average survival time).
  risk = -1*averageSurvivalTimes
  concordanceInfo = survConcordance(Surv(trueDeathTimes, censorStatus)~ risk)
  concordantPairs= concordanceInfo$stats[1] 
  discordantPairs = concordanceInfo$stats[2] 
  riskTies = concordanceInfo$stats[3]
  timeTies = concordanceInfo$stats[4]
  
  CIndex = switch(ties,
                  None = concordantPairs/(concordantPairs + discordantPairs),
                  Time = (concordantPairs+timeTies/2)/(concordantPairs + discordantPairs + timeTies),
                  Risk = (concordantPairs+riskTies/2)/(concordantPairs + discordantPairs + riskTies),
                  All = (concordantPairs+(riskTies +timeTies)/2)/(concordantPairs + discordantPairs + timeTies + riskTies)
  )
  return(CIndex)
}
```

**File Purpose**: Calculate the concordance of a trained model on test data.

**File Path**: "ISDEvaluationPipeline/Evaluations/Concordance.R"

**Functions**:

1. *Concordance(survMod, ties = "None", method = "Mean")*

**Arguments**:

1. *survMod*: The output of any of the survival models. Specifically this must be a list of (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. Technically the third component is not required for the concordance calcualtion.

2. *ties*: A string indicating the method in which to handle ties. There are four options: (1) "None" which will exclude all ties, (2) "Risk" which will include ties who were tied in their risk (mean/median survival time), (3) "Time" which will include ties whos survival time was equal, and (4) "All" which includes both risk and time ties.

3. *method*: A string indicating to use the "Mean" or the "Median" for the risk score evaluation. 

**Packages Required**: 

1. survival - This package is used for the *survConcordance()* function -- an time-efficient function for calcualting concordnace.

**File Notes**: 

As discussed in the paper, concordance is a discriminative measure typically used to evaluated models which produce risk scores for patients. Here the risk score is given by the **negative* of the mean/median of an individuals survival distribution. Once the average survival times are calculated *survConcordance()* calculated the concordance information and the concordance score is returned depending on the values of the *ties* argument.

**Example Usage**:

```{r, warning=FALSE}
#Set up:
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
CoxMod = CoxPH_KP(training1stFold, testing1stFold, ElasticNet=F)

#Evaluation:
Concordance(CoxMod, ties = "None", method = "Mean")

```

##1-Calibration (OneCalibration.R)

```{r, include= FALSE}
#We use this for the Surv function.
library(survival)

#We use this for group_by, summarise, and mutate.
library(dplyr)

#Helper Functions: predictProbabilityFromCurve(survivalCurve,predictedTimes, timeToPredict)
source("ISDEvaluationPipeline/Evaluations/EvaluationHelperFunctions.R")

OneCalibration = function(survMod, timeOfInterest = c(), type = "DN", numBuckets = 10){
  #Being passed an empty model.
  if(is.null(survMod)) return(NULL)
  #Being passed a model that failed.
  suppressWarnings(if(is.na(survMod[[1]])) return(NULL))
  if(!type %in% c("Uncensored","DN"))
    stop("Please enter one of 'Uncensored','DN' for type.")
  predictedTimes = survMod[[1]]$time
  survivalCurves = survMod[[1]][-1] #removes predicted times.
  trueDeathTimes = survMod[[2]]$time
  censorStatus = survMod[[2]]$delta
  trainingDeathTimes = survMod[[3]]$time
  trainingCensorStatus = survMod[[3]]$delta
  #If time is null, we will select the median from the KM curve generated from training instances.
  if(is.null(timeOfInterest)){
    timeOfInterest = unname(quantile(trueDeathTimes,c(.1,.25,.5,.75,.9)))
  }
  predictions = unlist(lapply(seq_along(trueDeathTimes),
                              function(index) predictProbabilityFromCurve(survivalCurves[,index],
                                                                          predictedTimes,
                                                                          timeOfInterest)))
  pval = c()
  for(times in 1:length(timeOfInterest)){
    #If we have multiple times of interest we need to pull out the predictions for a specific time.
    predictionsSpecified = predictions[seq(times,length(timeOfInterest)*length(trueDeathTimes),length(timeOfInterest))]
    pval = c(pval,binItUp(trueDeathTimes,censorStatus, predictionsSpecified, type, numBuckets,timeOfInterest[times]))
  }
  return(pval)
}

OneCalibrationCumulative = function(listOfSurvivalModels, timeOfInterest = c(), type = "DN", numBuckets = 10){
  if(length(listOfSurvivalModels) ==0) return(NULL)
  suppressWarnings(if(any(unlist(lapply(listOfSurvivalModels, is.na)))) return(NA))
  if(!type %in% c("Uncensored","DN"))
    stop("Please enter one of 'Uncensored','DN' for type.")
  
  predictedTimes = lapply(seq_along(listOfSurvivalModels), function(x) listOfSurvivalModels[[x]][[1]]$time)
  survivalCurves = lapply(seq_along(listOfSurvivalModels), function(x) listOfSurvivalModels[[x]][[1]][-1])
  trueDeathTimes = lapply(seq_along(listOfSurvivalModels), function(x) listOfSurvivalModels[[x]][[2]]$time)
  censorStatus   = lapply(seq_along(listOfSurvivalModels), function(x) listOfSurvivalModels[[x]][[2]]$delta)
  #If time is null, we will select the median from the training instances (of those who were uncensored.)
  if(is.null(timeOfInterest)){
    allTimes = unlist(trueDeathTimes)
    timeOfInterest = unname(quantile(allTimes,c(.1,.25,.5,.75,.9)))
  }
  pval = c()
  for(times in 1:length(timeOfInterest)){
    predictions = unlist(lapply(seq_along(listOfSurvivalModels), function(model) lapply(seq_along(trueDeathTimes[[model]]),
                                                                                        function(index) predictProbabilityFromCurve(survivalCurves[[model]][,index],
                                                                                                                                    predictedTimes[[model]],
                                                                                                                                    timeOfInterest[times]))))
    pval = c(pval,binItUp(unlist(trueDeathTimes),unlist(censorStatus), predictions, type, numBuckets,timeOfInterest[times]))
  }
  return(pval)
}


binItUp = function(trueDeathTimes,censorStatus, predictions, type, numBuckets,timeOfInterest){
  #We need to divide the number of predictions into as equal size buckets as possible. The formula for this can be found here:
  #https://math.stackexchange.com/questions/199690/divide-n-items-into-m-groups-with-as-near-equal-size-as-possible
  numberOfBucketsWithExtra = length(predictions) %% numBuckets
  numberOfBucketsWithoutExtra = numBuckets - numberOfBucketsWithExtra
  #We want buckets to randomly be assigned their respective number of predictions, but be consistent with the same input.
  #E.g. for 3 buckets with size 5 and 2 buckets of size 4 we don't necasarrily want 5,5,5,4,4. We want them to be random. We use sample().
  set.seed(42)
  bucketSizes = sample(c(rep(floor(length(predictions)/numBuckets)+1, numberOfBucketsWithExtra),
                         rep(floor(length(predictions)/numBuckets), numberOfBucketsWithoutExtra)))
  bucketLabel = rep(1:numBuckets, times = bucketSizes)
  
  #We are going to put the survival probabilities into deciles so we get the order of indexs here.
  orderPredictions = order(predictions)
  orderedPredictionFrame = data.frame(prob = predictions[orderPredictions],
                                      label = bucketLabel,
                                      delta = censorStatus[orderPredictions],
                                      time = trueDeathTimes[orderPredictions])
  timeFrame = data.frame(time = trueDeathTimes[orderPredictions],
                         label = bucketLabel,
                         delta = censorStatus[orderPredictions])
  pval = switch(type,
                Uncensored = {
                  #Read the following code as taking our predicted probabilities, placing them into buckets, 
                  #taking only uncensored individuals at the time of interest and then taking their mean. All other following piping operations should
                  #be read in a similar fashion.
                  bucketSurvival = orderedPredictionFrame  %>%
                    group_by(label) %>% 
                    filter(delta ==1 | (delta ==0 & time >= timeOfInterest)) %>%
                    summarise(survivalPrediction = mean(prob))
                  numDied = timeFrame  %>%
                    group_by(label) %>%
                    filter(delta ==1| (delta ==0 & time >= timeOfInterest)) %>%
                    summarise(deadCount = length(which(time < timeOfInterest)))
                  observed = numDied$deadCount
                  expected = bucketSizes*(1-bucketSurvival$survivalPrediction)
                  HLStat = sum((observed-expected)^2/(bucketSizes*(1-bucketSurvival$survivalPrediction)*bucketSurvival$survivalPrediction))
                  pval = 1-pchisq(HLStat, numBuckets-2)
                },
                #For a good discusion of this method, also known as the D'Agostino-Nam tranlation/method see
                #Section 2.2 of this thesis paper: Hosmer-Lemeshow goodness-of-fit test: Translations to the Cox Proportional Hazards Model
                DN = {
                  bucketSurvival = orderedPredictionFrame  %>%
                    group_by(label) %>%
                    summarise(survivalPrediction = mean(prob))
                  numDied = timeFrame  %>%
                    group_by(label) %>%
                    mutate(slope = (1-min(prodlim(Surv(time, delta)~1)$surv))/(0-max(time))) %>%
                    summarise(deadCount = 1-ifelse(is.na(predict(prodlim(Surv(time, delta)~1),timeOfInterest)),max(0,1+slope*timeOfInterest),
                                                     predict(prodlim(Surv(time, delta)~1),timeOfInterest)))
                  observed = numDied$deadCount
                  expected = 1-bucketSurvival$survivalPrediction
                  HLStat = (bucketSizes[bucketSizes!=0]*(observed-expected)^2)/((1-bucketSurvival$survivalPrediction)*bucketSurvival$survivalPrediction)
                  HLStat = sum(ifelse(is.nan(HLStat),0,HLStat))
                  #See comments in file header for reasoning of degree of freedom choice.
                  DoF = ifelse(numBuckets > 15, numBuckets -2, numBuckets-1)
                  pval = 1-pchisq(HLStat, DoF)
                })
  return(pval)
}
```

**File Purpose**: Perform the 1-Calibration test on survival data.

**File Path**: "ISDEvaluationPipeline/Evaluations/OneCalibration.R"

**Functions**:

1. *OneCalibration(survMod, timeOfInterest = c(), type = "DN", numBuckets = 10)*
2. *OneCalibrationCumulative(listOfSurvivalModels, timeOfInterest = c(), type = "DN", numBuckets = 10)*
3. *binItUp(trueDeathTimes,censorStatus, predictions, type, numBuckets,timeOfInterest)*

**Arguments**:

1. *survMod*: The output of any of the survival models. Specifically this must be a list of (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. Technically the third component is not required for the concordance calcualtion.

2. *timeOfInterest*: The time at which to evaluate the model for 1-Calibration. If nothing is passed in the quantiles (.1, .25,.5,.75,.9) of the true evennt times will be used.

3. *type*: This is the type of 1-Calibration to be used, either "Uncensored" to use 1-Calibration for only the uncensored patients and discard all censored patients or "DN" for the D'Agostino-Nam translation.

4. *numBuckets*: The number of buckets/bins to use for 1-Calibration.

5. *listOfSurvivalModels*: A list of the output of survival models, *e.g.* a list of different *survMod* like objects.

6. *trueDeathTimes*: The true event times of the subjects.

7. *censorStatus*: The bit identifying if a patient died or was censored, *i.e* **delta**.

8. *predictions*: The predicted survival probability for patients at their true event times.

**Packages Required**: 

1. survival -- This package is used for the *Surv()* function.

2. dplyr -- This is used for *group_by()*, *summarise()*, aand *mutate()*.

**File Notes**: 

There are two ways to perform 1-Calibration here -- one can either pass the output from a survival model and have 1-Calibration evaluated on the test fold or one can pass a list of multiple survival models and have 1-Calibration evaluated on all patients. Note by doing the first way, *OneCalibration()*, there will end up being the same number of p-values as there are cross-validation folds and there will be no straightforward way of combining them (DO NOT AVERAGE THEM). *OneCalibrationCumulative()* mitigates this by taking in the model results from all folds and performing a single 1-Calibration test using all the test data. *binItUp()* is a helper function that takes probability predictions and performs the actual specified 1-Calibration test.

**Example Usage**:

```{r, warning=FALSE}
#Set up for OneCalibration():
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
CoxMod = CoxPH_KP(training1stFold, testing1stFold, ElasticNet=F)

#Evaluation:
#By not specifying timeOfInterest we will get the 10th, 25th, 50th, 75th, and 90th quantiles of the true event times in the testing fold.
OneCalibration(CoxMod, timeOfInterest = c(), type = "DN",numBuckets = 10)

#One Calibration for uncensored patients.
OneCalibration(CoxMod, timeOfInterest = c(), type = "Uncensored",numBuckets = 10)

#Set up for OneCalibrationCumulative():
coxList = list()
for(i in 1:5){
  trainingFold = trainAndTest$Training[[i]]
  testingFold = trainAndTest$Testing[[i]]
  CoxMod = CoxPH_KP(trainingFold, testingFold, ElasticNet=F)
  coxList[[i]] = CoxMod
}
#Cumulative 1-Calibration for default time of Interest
OneCalibrationCumulative(listOfSurvivalModels = coxList, timeOfInterest = c(), type = "DN", numBuckets = 10)
#Cumulative 1-Calibration for a specific time (2 months)
OneCalibrationCumulative(listOfSurvivalModels = coxList, timeOfInterest = 2, type = "DN", numBuckets = 10)
```


##Brier Score (BrierScore.R)

```{r, include=FALSE}
#Library dependencies:
#prodlim gives a faster KM implementation and also gives a predict function for KM
library(prodlim)
#Helper functions:
source("ISDEvaluationPipeline/Evaluations/EvaluationHelperFunctions.R")

BrierScore = function(survMod, type = "Integrated", singleBrierTime = NULL, integratedBrierTimes = NULL, numPoints=NULL){
  #Being passed an empty model.
  if(is.null(survMod)) return(NULL)
  #Being passed a model that failed.
  suppressWarnings(if(is.na(survMod[[1]])) return(NULL))
  if(type == "Integrated" & is.null(integratedBrierTimes))
    integratedBrierTimes = c(0, max(c(survMod[[2]]$time, survMod[[3]]$time))) #max time of the training and testing set combined (the entire dataset).
  score = ifelse(type =="Single",
                 singleBrier(survMod, singleBrierTime),
                 integratedBrier(survMod, integratedBrierTimes, numPoints))
  return(score)
}

singleBrier = function(survMod, singleBrierTime){
  eventTimes = survMod[[2]]$time
  censorStatus = survMod[[2]]$delta
  trainingEventTimes = survMod[[3]]$time
  trainingCensorStatus = survMod[[3]]$delta
  #Default brier time will be the 50th quantile of training and testing event times combined.
  if(is.null(singleBrierTime)){
    singleBrierTime = quantile(c(eventTimes, trainingEventTimes), .5)
  }
  inverseCensorTrain = 1 - trainingCensorStatus
  invProbCensor = prodlim(Surv(trainingEventTimes,inverseCensorTrain)~1)
  #Here we are ordering event times and then using predict with level.chaos = 1 which returns predictions ordered by time.
  orderOfTimes = order(eventTimes)
  #Category one is individuals with event time lower than the time of interest and were NOT censored.
  weightCat1 = (eventTimes[orderOfTimes] <= singleBrierTime & censorStatus[orderOfTimes])*predict(invProbCensor,
                                                                                            eventTimes,
                                                                                            level.chaos = 1)
  #Catch if event times goes over max training event time, i.e. predict gives NA
  weightCat1[is.na(weightCat1)] = 0
  #Category 2 is individuals whose time was greater than the time of interest (singleBrierTime) - both censored and uncensored individuals.
  weightCat2 = (eventTimes[orderOfTimes] > singleBrierTime)*predict(invProbCensor,
                                                                    singleBrierTime,
                                                              level.chaos = 1)
  #predict returns NA if the passed in time is greater than any of the times used to build the inverse probability of censoring model.
  weightCat2[is.na(weightCat2)] = 0
  
  predictedTimes = survMod[[1]][,1]
  #Take the survival curves, remove the times column, and then order the curves by the order in which they died. We have to order them to 
  #line up with the weight vectors.
  survivalCurvesOrdered = survMod[[1]][,-1][orderOfTimes]
  predictions = apply(survivalCurvesOrdered,2, function(z) predictProbabilityFromCurve(z,predictedTimes,singleBrierTime))
  bScore = mean(predictions^2*weightCat1 + (1-predictions)^2*weightCat2)
  return(bScore)
}

#integrated Brier Score based on the single time point brier score, found above.
integratedBrier = function(survMod, integratedBrierTimes,numPoints){
  censorStatus = survMod[[2]]$delta
  eventTimes = survMod[[2]]$time[censorStatus==1]
  orderOfTimes = order(eventTimes)
  sortedEvents = sort(eventTimes)
  if(is.null(numPoints)){
    points = sortedEvents[sortedEvents >= integratedBrierTimes[1] & sortedEvents <=integratedBrierTimes[2]]
  } else{
    points = seq(integratedBrierTimes[1], integratedBrierTimes[2], length.out = numPoints)
  }
  bscores = singleBrierMultiplePoints(survMod,points)
  indexs = 2:length(points)
  trapezoidIntegralVal = diff(points) %*% ((bscores[indexs - 1] + bscores[indexs])/2)
  intBScore = trapezoidIntegralVal/diff(range(points))
  return(intBScore)
}

singleBrierMultiplePoints = function(survMod, BrierTimes){
  eventTimes = survMod[[2]]$time
  censorStatus = survMod[[2]]$delta
  trainingEventTimes = survMod[[3]]$time
  trainingCensorStatus = survMod[[3]]$delta
  inverseCensorTrain = 1 - trainingCensorStatus
  invProbCensor = prodlim(Surv(trainingEventTimes,inverseCensorTrain)~1)
  orderOfTimes = order(eventTimes)

  bsPointsMat = matrix(rep(BrierTimes, length(eventTimes)), nrow = length(eventTimes),byrow = T)
  
  #Each column represents the indicator for a single brier score 
  weightCat1Mat = (eventTimes[orderOfTimes] <= bsPointsMat & censorStatus[orderOfTimes])*predict(invProbCensor, eventTimes, level.chaos = 1)
  #Catch if event times goes over max training event time, i.e. predict gives NA
  weightCat1Mat[is.na(weightCat1Mat)] = 0

  weightCat2Mat = t(t((eventTimes[orderOfTimes] > bsPointsMat))*predict(invProbCensor, BrierTimes,level.chaos = 2))
  #Catch if BrierTimes goes over max event time, i.e. predict gives NA
  weightCat2Mat[is.na(weightCat2Mat)] = 0
  
  predictedTimes = survMod[[1]][,1]
  #Take the survival curves, remove the times column, and then order the curves by the order in which they died. We have to order them to
  #line up with the weight matricies.
  survivalCurvesOrdered = survMod[[1]][,-1][orderOfTimes]
  predictions = t(apply(survivalCurvesOrdered,2, function(curve) predictProbabilityFromCurve(curve,predictedTimes,BrierTimes)))
  bscores = apply(predictions^2*weightCat1Mat + (1-predictions)^2*weightCat2Mat, 2, mean)
  return(bscores)
}
```

**File Purpose**: Evaluate the Brier score on test data.

**File Path**: "ISDEvaluationPipeline/Evaluations/BrierScore.R"

**Functions**:

1. *BrierScore(survMod. type = "Interated", singleTime = NULL, integratedBrierTimes = NULL, numPoints = NULL)*
2. *singleBrier(survMod, BrierTime = NULL)*
3. *integratedBrier(survMod, integratedBrierTimes, numPoints)*
4. *sinleBrierMultiplePoints(survMod, BrierTimes)*

**Arguments**:

1. *survMod*: The output of any of the survival models. Specifically this must be a list of (1) the survival curves of the test patients, (2) the test patients true time of death and censor status, and (3) the training patients true time of death and censor status. Technically the third component is not required for the concordance calcualtion.

2. *type*: A string of either "Integrated" or "Single". This will indicate whether to return the Brier score for a point or the integrated Brier score.

3. *singleBrierTime*: The time point to evaluate the (non-integrated) Brier score. If left as NULL, the 50th percentile of the training and testing data's event times combined will be used.

4. *integratedBrierTimes*: A vector of length 2 indicating the limits of integration for the integrated Brier score. If left as NULL the range will be from zero to the max event time of the entire dataset (training and testing set combined).

5. *numPoints*: The number of points to evaluate the integral, *i.e.* the higher the number of points the less error in the integration. If left as NULL the number of points will correspond to the number of events between the limits of integration.

6. *BrierTimes*: A vector of times to evaluate a single Brier score -- used in an internal function (*singleBrierMultiplePoints()*) of *integratedBrier()*.

**Packages Required**: 

1. prodlim -- Used for the *prodlim()* function that calcuates the Kaplan-Meier curve. This is then used for the inverse probability of censoring weights (IPCW).

**File Notes**: 

In this file *BrierScore()* is the only function that needs to be called -- all the other functions simply work within *BrierScore()*. Note that there is no option for removing censored patients and the Brier score returned will always be the score corrected by the IPCWs. For the integrated Brier score the integral is evaluated using the simple [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule). If numPoints is specified then the points where the Brier score is evaluated will be evenly spread across the limits of integration. However, if numPoints is not specified then the points evaluated will be the event times in the testing data. These rules follow from the *sbrier()* function in the ipred library written by Torsten Hothorn.

**Example Usage**:


```{r, warning=FALSE}
#Set up:
validatedAndCleaned = validateAndClean(survivalDataset = NACD)
folds = createFoldsAndNormalize(survivalDataset = validatedAndCleaned, numberOfFolds = 5)
trainAndTest = folds[[2]]
training1stFold = trainAndTest$Training[[1]]
testing1stFold = trainAndTest$Testing[[1]]
CoxMod = CoxPH_KP(training1stFold, testing1stFold, ElasticNet=F)

#Evaluation:
#Integrated with specified times:
BrierScore(CoxMod, type = "Integrated", integratedBrierTimes = c(0,36))

#Integrated with unspecified times:
BrierScore(CoxMod, type = "Integrated")

#Single with specified times:
BrierScore(CoxMod, type = "Single", singleBrierTime = 12)

#Single with unspecified times:
BrierScore(CoxMod, type = "Single")
```

##L1-Loss (L1Measures.R) 


##D-Calibration (DCalibration.R)



#Miscellanious

##Plotting Individual Survival Curves (plotSurvivalCurves.R)

##Feature Selection (FeatureSelection.R)

#The Code - From Beginning To End 
























